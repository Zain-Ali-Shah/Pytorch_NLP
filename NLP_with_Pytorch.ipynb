{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlU3fZlyKNBI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/Deep_learning-datasets/shakespeare.txt','r',encoding='utf8') as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "LYnuMqi0Kkdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udzcMM7eLCFw",
        "outputId": "5dc4c26b-62e6-4fb2-c428-8776b5acc6e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIOK_61OLD6q",
        "outputId": "2f8177be-5300-4c56-a611-0e896c3260d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "                     1\n",
            "  From fairest creatures we desire increase,\n",
            "  That thereby beauty's rose might never die,\n",
            "  But as the riper should by time decease,\n",
            "  His tender heir might bear his memory:\n",
            "  But thou contracted to thine own bright eyes,\n",
            "  Feed'st thy light's flame with self-substantial fuel,\n",
            "  Making a famine where abundance lies,\n",
            "  Thy self thy foe, to thy sweet self too cruel:\n",
            "  Thou that art now the world's fresh ornament,\n",
            "  And only herald to the gaudy spring,\n",
            "  Within thine own bud buriest thy content,\n",
            "  And tender churl mak'st waste in niggarding:\n",
            "    Pity the world, or else this glutton be,\n",
            "    To eat the world's due, by the grave and thee.\n",
            "\n",
            "\n",
            "                     2\n",
            "  When forty winters shall besiege thy brow,\n",
            "  And dig deep trenches in thy beauty's field,\n",
            "  Thy youth's proud livery so gazed on now,\n",
            "  Will be a tattered weed of small worth held:  \n",
            "  Then being asked, where all thy beauty lies,\n",
            "  Where all the treasure of thy lusty days;\n",
            "  To say within thine own deep su\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maA30blHLFwi",
        "outputId": "b1ffe8a3-e57a-4977-9406-04b5fa163c5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5445609"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_characters = set(text)"
      ],
      "metadata": {
        "id": "9Ur7eBnhLiKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(all_characters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8vU1zd_MEGn",
        "outputId": "40e3363d-2cd6-426f-ad3a-452424f094da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "84"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = dict(enumerate(all_characters))"
      ],
      "metadata": {
        "id": "X-s3NORAMGDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJB6oQAQMrU1",
        "outputId": "96964399-6e9a-4fb9-ada4-faaa95555dd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'n',\n",
              " 1: 'f',\n",
              " 2: 'J',\n",
              " 3: 'g',\n",
              " 4: '8',\n",
              " 5: 'w',\n",
              " 6: 'l',\n",
              " 7: 'd',\n",
              " 8: '|',\n",
              " 9: 'y',\n",
              " 10: 'c',\n",
              " 11: 'a',\n",
              " 12: 'R',\n",
              " 13: 'O',\n",
              " 14: '\"',\n",
              " 15: '4',\n",
              " 16: 'F',\n",
              " 17: '7',\n",
              " 18: 'M',\n",
              " 19: 'G',\n",
              " 20: ']',\n",
              " 21: '6',\n",
              " 22: 'I',\n",
              " 23: '1',\n",
              " 24: '0',\n",
              " 25: ':',\n",
              " 26: 'Z',\n",
              " 27: 'h',\n",
              " 28: 't',\n",
              " 29: 'o',\n",
              " 30: '?',\n",
              " 31: '`',\n",
              " 32: 'B',\n",
              " 33: '[',\n",
              " 34: '\\n',\n",
              " 35: 'U',\n",
              " 36: '9',\n",
              " 37: ')',\n",
              " 38: 'r',\n",
              " 39: '_',\n",
              " 40: 'P',\n",
              " 41: 'N',\n",
              " 42: 'k',\n",
              " 43: '5',\n",
              " 44: 'q',\n",
              " 45: ',',\n",
              " 46: 'i',\n",
              " 47: 'L',\n",
              " 48: 'S',\n",
              " 49: 'm',\n",
              " 50: 'H',\n",
              " 51: 'Y',\n",
              " 52: 'V',\n",
              " 53: \"'\",\n",
              " 54: '(',\n",
              " 55: '2',\n",
              " 56: 's',\n",
              " 57: 'b',\n",
              " 58: 'e',\n",
              " 59: 'p',\n",
              " 60: 'v',\n",
              " 61: '&',\n",
              " 62: 'E',\n",
              " 63: 'A',\n",
              " 64: 'Q',\n",
              " 65: 'D',\n",
              " 66: '.',\n",
              " 67: 'K',\n",
              " 68: '}',\n",
              " 69: '-',\n",
              " 70: ';',\n",
              " 71: 'C',\n",
              " 72: 'T',\n",
              " 73: 'X',\n",
              " 74: 'W',\n",
              " 75: 'x',\n",
              " 76: 'u',\n",
              " 77: ' ',\n",
              " 78: '<',\n",
              " 79: '3',\n",
              " 80: '>',\n",
              " 81: 'z',\n",
              " 82: 'j',\n",
              " 83: '!'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = {char:ind for ind,char in decoder.items()}"
      ],
      "metadata": {
        "id": "Rug2L1p6MsZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CfCxibHNTer",
        "outputId": "c9c26036-d011-44d8-fd4c-3768caf4e153"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'n': 0,\n",
              " 'f': 1,\n",
              " 'J': 2,\n",
              " 'g': 3,\n",
              " '8': 4,\n",
              " 'w': 5,\n",
              " 'l': 6,\n",
              " 'd': 7,\n",
              " '|': 8,\n",
              " 'y': 9,\n",
              " 'c': 10,\n",
              " 'a': 11,\n",
              " 'R': 12,\n",
              " 'O': 13,\n",
              " '\"': 14,\n",
              " '4': 15,\n",
              " 'F': 16,\n",
              " '7': 17,\n",
              " 'M': 18,\n",
              " 'G': 19,\n",
              " ']': 20,\n",
              " '6': 21,\n",
              " 'I': 22,\n",
              " '1': 23,\n",
              " '0': 24,\n",
              " ':': 25,\n",
              " 'Z': 26,\n",
              " 'h': 27,\n",
              " 't': 28,\n",
              " 'o': 29,\n",
              " '?': 30,\n",
              " '`': 31,\n",
              " 'B': 32,\n",
              " '[': 33,\n",
              " '\\n': 34,\n",
              " 'U': 35,\n",
              " '9': 36,\n",
              " ')': 37,\n",
              " 'r': 38,\n",
              " '_': 39,\n",
              " 'P': 40,\n",
              " 'N': 41,\n",
              " 'k': 42,\n",
              " '5': 43,\n",
              " 'q': 44,\n",
              " ',': 45,\n",
              " 'i': 46,\n",
              " 'L': 47,\n",
              " 'S': 48,\n",
              " 'm': 49,\n",
              " 'H': 50,\n",
              " 'Y': 51,\n",
              " 'V': 52,\n",
              " \"'\": 53,\n",
              " '(': 54,\n",
              " '2': 55,\n",
              " 's': 56,\n",
              " 'b': 57,\n",
              " 'e': 58,\n",
              " 'p': 59,\n",
              " 'v': 60,\n",
              " '&': 61,\n",
              " 'E': 62,\n",
              " 'A': 63,\n",
              " 'Q': 64,\n",
              " 'D': 65,\n",
              " '.': 66,\n",
              " 'K': 67,\n",
              " '}': 68,\n",
              " '-': 69,\n",
              " ';': 70,\n",
              " 'C': 71,\n",
              " 'T': 72,\n",
              " 'X': 73,\n",
              " 'W': 74,\n",
              " 'x': 75,\n",
              " 'u': 76,\n",
              " ' ': 77,\n",
              " '<': 78,\n",
              " '3': 79,\n",
              " '>': 80,\n",
              " 'z': 81,\n",
              " 'j': 82,\n",
              " '!': 83}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_text = np.array([encoder[char] for char in text])"
      ],
      "metadata": {
        "id": "ZmYPLSGINWkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_text[:500]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgDDGvQhOHfA",
        "outputId": "f55e98aa-a5d4-425e-9b48-78e7ec7291c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([34, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77,\n",
              "       77, 77, 77, 77, 77, 23, 34, 77, 77, 16, 38, 29, 49, 77,  1, 11, 46,\n",
              "       38, 58, 56, 28, 77, 10, 38, 58, 11, 28, 76, 38, 58, 56, 77,  5, 58,\n",
              "       77,  7, 58, 56, 46, 38, 58, 77, 46,  0, 10, 38, 58, 11, 56, 58, 45,\n",
              "       34, 77, 77, 72, 27, 11, 28, 77, 28, 27, 58, 38, 58, 57,  9, 77, 57,\n",
              "       58, 11, 76, 28,  9, 53, 56, 77, 38, 29, 56, 58, 77, 49, 46,  3, 27,\n",
              "       28, 77,  0, 58, 60, 58, 38, 77,  7, 46, 58, 45, 34, 77, 77, 32, 76,\n",
              "       28, 77, 11, 56, 77, 28, 27, 58, 77, 38, 46, 59, 58, 38, 77, 56, 27,\n",
              "       29, 76,  6,  7, 77, 57,  9, 77, 28, 46, 49, 58, 77,  7, 58, 10, 58,\n",
              "       11, 56, 58, 45, 34, 77, 77, 50, 46, 56, 77, 28, 58,  0,  7, 58, 38,\n",
              "       77, 27, 58, 46, 38, 77, 49, 46,  3, 27, 28, 77, 57, 58, 11, 38, 77,\n",
              "       27, 46, 56, 77, 49, 58, 49, 29, 38,  9, 25, 34, 77, 77, 32, 76, 28,\n",
              "       77, 28, 27, 29, 76, 77, 10, 29,  0, 28, 38, 11, 10, 28, 58,  7, 77,\n",
              "       28, 29, 77, 28, 27, 46,  0, 58, 77, 29,  5,  0, 77, 57, 38, 46,  3,\n",
              "       27, 28, 77, 58,  9, 58, 56, 45, 34, 77, 77, 16, 58, 58,  7, 53, 56,\n",
              "       28, 77, 28, 27,  9, 77,  6, 46,  3, 27, 28, 53, 56, 77,  1,  6, 11,\n",
              "       49, 58, 77,  5, 46, 28, 27, 77, 56, 58,  6,  1, 69, 56, 76, 57, 56,\n",
              "       28, 11,  0, 28, 46, 11,  6, 77,  1, 76, 58,  6, 45, 34, 77, 77, 18,\n",
              "       11, 42, 46,  0,  3, 77, 11, 77,  1, 11, 49, 46,  0, 58, 77,  5, 27,\n",
              "       58, 38, 58, 77, 11, 57, 76,  0,  7, 11,  0, 10, 58, 77,  6, 46, 58,\n",
              "       56, 45, 34, 77, 77, 72, 27,  9, 77, 56, 58,  6,  1, 77, 28, 27,  9,\n",
              "       77,  1, 29, 58, 45, 77, 28, 29, 77, 28, 27,  9, 77, 56,  5, 58, 58,\n",
              "       28, 77, 56, 58,  6,  1, 77, 28, 29, 29, 77, 10, 38, 76, 58,  6, 25,\n",
              "       34, 77, 77, 72, 27, 29, 76, 77, 28, 27, 11, 28, 77, 11, 38, 28, 77,\n",
              "        0, 29,  5, 77, 28, 27, 58, 77,  5, 29, 38,  6,  7, 53, 56, 77,  1,\n",
              "       38, 58, 56, 27, 77, 29, 38,  0, 11, 49, 58,  0, 28, 45, 34, 77, 77,\n",
              "       63,  0,  7, 77, 29,  0,  6,  9, 77, 27, 58, 38, 11,  6,  7, 77, 28,\n",
              "       29, 77, 28, 27, 58, 77,  3, 11, 76,  7,  9, 77, 56, 59, 38, 46,  0,\n",
              "        3, 45, 34, 77, 77, 74, 46, 28, 27, 46,  0, 77, 28, 27, 46,  0, 58,\n",
              "       77, 29,  5,  0, 77, 57, 76])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder[54]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NN2arzdVOLfH",
        "outputId": "ef74f1c9-587f-45b4-fafc-105920972888"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'('"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encoder(encoded_text,num_uni_chars):\n",
        "  one_hot = np.zeros((encoded_text.size,num_uni_chars))\n",
        "  one_hot = one_hot.astype(np.float32)\n",
        "  one_hot[np.arange(one_hot.shape[0]),encoded_text.flatten()] = 1.0\n",
        "  one_hot = one_hot.reshape((*encoded_text.shape,num_uni_chars))\n",
        "  return one_hot"
      ],
      "metadata": {
        "id": "kpNANoOWW_fe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr = np.array([1,2,0])"
      ],
      "metadata": {
        "id": "7V1th765YPF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_encoder(arr,3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjcitsrNYocR",
        "outputId": "5dd91673-39c3-4c28-8191-43e43f4a9719"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_text = np.arange(10)"
      ],
      "metadata": {
        "id": "jhepwyseYsf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lU2XjM2LlHfB",
        "outputId": "8e3b7800-8d50-4a85-fb0d-ef012eb32451"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_text.reshape((5,-1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea3gG7nXlKX1",
        "outputId": "ceb96a33-902e-4471-ee16-2e005030f2ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1],\n",
              "       [2, 3],\n",
              "       [4, 5],\n",
              "       [6, 7],\n",
              "       [8, 9]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_batches(encoded_text, samp_per_batch=10, seq_len=50):\n",
        "    char_per_batch = samp_per_batch * seq_len\n",
        "    num_batches_avail = int(len(encoded_text)/char_per_batch)\n",
        "    encoded_text = encoded_text[:num_batches_avail * char_per_batch]\n",
        "    encoded_text = encoded_text.reshape((samp_per_batch, -1))\n",
        "    for n in range(0, encoded_text.shape[1], seq_len):\n",
        "        x = encoded_text[:, n:n+seq_len]\n",
        "        y = np.zeros_like(x)\n",
        "        try:\n",
        "            y[:, :-1] = x[:, 1:]\n",
        "            y[:, -1]  = encoded_text[:, n+seq_len]\n",
        "        except:\n",
        "            y[:, :-1] = x[:, 1:]\n",
        "            y[:, -1] = encoded_text[:, 0]\n",
        "        yield x, y"
      ],
      "metadata": {
        "id": "d4yCf6p3lQu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = np.arange(20)"
      ],
      "metadata": {
        "id": "CbBhB7lL07Bg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IkO8u2q1Zxq",
        "outputId": "9111bb73-960b-4f43-cf0f-729f1f041cb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
              "       17, 18, 19])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_generator = generate_batches(sample_text,samp_per_batch=2,seq_len=5)"
      ],
      "metadata": {
        "id": "4Jfv9KAB1b6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x,y = next(batch_generator)"
      ],
      "metadata": {
        "id": "EyNf7rAg1oIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qqRusXj1rWY",
        "outputId": "ff0106d3-41c2-42d0-93ba-941c44534782"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  1,  2,  3,  4],\n",
              "       [10, 11, 12, 13, 14]])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uqujuv4g1siP",
        "outputId": "f5c6fe1a-9c0c-4424-b140-71651b5d9ed0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  2,  3,  4,  5],\n",
              "       [11, 12, 13, 14, 15]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CharModel(nn.Module):\n",
        "  def __init__(self,all_chars,num_hidden=256,num_layers=4,drop_prob=0.5,use_gpu=False):\n",
        "    super().__init__()\n",
        "    self.drop_prob = drop_prob\n",
        "    self.num_layers = num_layers\n",
        "    self.num_hidden = num_hidden\n",
        "    self.use_gpu = use_gpu\n",
        "    self.all_chars = all_chars\n",
        "    self.decoder = dict(enumerate(all_chars))\n",
        "    self.encoder = {char:ind for ind,char in decoder.items()}\n",
        "    self.lstm = nn.LSTM(len(self.all_chars),num_hidden,num_layers,dropout=drop_prob,batch_first=True)\n",
        "    self.dropout = nn.Dropout(drop_prob)\n",
        "    self.fc_linear = nn.Linear(num_hidden,len(self.all_chars))\n",
        "  def forward(self,x,hidden):\n",
        "    lstm_output,hidden = self.lstm(x,hidden)\n",
        "    drop_output = self.dropout(lstm_output)\n",
        "    drop_output = drop_output.contiguous().view(-1,self.num_hidden)\n",
        "    final_out = self.fc_linear(drop_output)\n",
        "    return final_out,hidden\n",
        "  def hidden_state(self,batch_size):\n",
        "    if self.use_gpu:\n",
        "      hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda(),\n",
        "                torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda())\n",
        "    else:\n",
        "      hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden),\n",
        "                torch.zeros(self.num_layers,batch_size,self.num_hidden))\n",
        "    return hidden"
      ],
      "metadata": {
        "id": "P5-eMiRZ5Ogb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CharModel(all_chars=all_characters,num_hidden=512,num_layers=3,drop_prob=0.5,use_gpu=True)"
      ],
      "metadata": {
        "id": "ZO2B2IS2d8qq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_param = []\n",
        "for p in model.parameters():\n",
        "  total_param.append(int(p.numel()))"
      ],
      "metadata": {
        "id": "R1ZWxUWYe8xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum(total_param)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsREQTZufJOd",
        "outputId": "8844549f-e40b-4a3f-c505-1652a474fbb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5470292"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "FQVBIxmtfKQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_percent = 0.9"
      ],
      "metadata": {
        "id": "GjO45q_dhLEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ind = int(len(encoded_text) * (train_percent))"
      ],
      "metadata": {
        "id": "rgAs1kHihSK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = encoded_text[:train_ind]\n",
        "val_data = encoded_text[train_ind:]"
      ],
      "metadata": {
        "id": "qHpDUXrMhabC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMfk3HUPhmWq",
        "outputId": "72e8ec6d-109f-4689-d138-71f1c743d997"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4901048"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(val_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qRnKp-whpTo",
        "outputId": "4543dcce-ee3e-4fdd-8952-47842f4b8551"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "544561"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 60\n",
        "batch_size = 100\n",
        "seq_len = 100\n",
        "tracker = 0\n",
        "num_char = max(encoded_text) + 1"
      ],
      "metadata": {
        "id": "O5EKi5T9hqUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "if model.use_gpu:\n",
        "  model.cuda()\n",
        "for i in range(epochs):\n",
        "  hidden = model.hidden_state(batch_size)\n",
        "  for x,y in generate_batches(train_data,batch_size,seq_len):\n",
        "    tracker += 1\n",
        "    x = one_hot_encoder(x,num_char)\n",
        "    inputs = torch.from_numpy(x)\n",
        "    targets = torch.from_numpy(y)\n",
        "    if model.use_gpu:\n",
        "      inputs = inputs.cuda()\n",
        "      targets = targets.cuda()\n",
        "    hidden = tuple([state.data for state in hidden])\n",
        "    model.zero_grad()\n",
        "    lstm_output,hidden = model.forward(inputs,hidden)\n",
        "    loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(),max_norm=5)\n",
        "    optimizer.step()\n",
        "    if tracker % 25 == 0:\n",
        "      val_hidden = model.hidden_state(batch_size)\n",
        "      val_losses = []\n",
        "      model.eval()\n",
        "      for x,y in generate_batches(val_data,batch_size,seq_len):\n",
        "        x = one_hot_encoder(x,num_char)\n",
        "        inputs = torch.from_numpy(x)\n",
        "        targets = torch.from_numpy(y)\n",
        "        if model.use_gpu:\n",
        "          inputs = inputs.cuda()\n",
        "          targets = targets.cuda()\n",
        "        val_hidden = tuple([state.data for state in val_hidden])\n",
        "        lstm_output,val_hidden = model.forward(inputs,val_hidden)\n",
        "        val_loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())\n",
        "        val_losses.append(val_loss.item())\n",
        "      model.train()\n",
        "      print(f\"EPOCH: {i} STEP: {tracker} VAL_LOSS: {val_loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rM1s1kDjrsm6",
        "outputId": "98db2bf4-40a1-4b43-c2d6-b6243cb48cc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 0 STEP: 25 VAL_LOSS: 3.205142021179199\n",
            "EPOCH: 0 STEP: 50 VAL_LOSS: 3.1936755180358887\n",
            "EPOCH: 0 STEP: 75 VAL_LOSS: 3.1919937133789062\n",
            "EPOCH: 0 STEP: 100 VAL_LOSS: 3.189558744430542\n",
            "EPOCH: 0 STEP: 125 VAL_LOSS: 3.074559450149536\n",
            "EPOCH: 0 STEP: 150 VAL_LOSS: 2.9705491065979004\n",
            "EPOCH: 0 STEP: 175 VAL_LOSS: 2.8625385761260986\n",
            "EPOCH: 0 STEP: 200 VAL_LOSS: 2.7379813194274902\n",
            "EPOCH: 0 STEP: 225 VAL_LOSS: 2.6514999866485596\n",
            "EPOCH: 0 STEP: 250 VAL_LOSS: 2.5337698459625244\n",
            "EPOCH: 0 STEP: 275 VAL_LOSS: 2.4220330715179443\n",
            "EPOCH: 0 STEP: 300 VAL_LOSS: 2.32315731048584\n",
            "EPOCH: 0 STEP: 325 VAL_LOSS: 2.2567105293273926\n",
            "EPOCH: 0 STEP: 350 VAL_LOSS: 2.194070816040039\n",
            "EPOCH: 0 STEP: 375 VAL_LOSS: 2.145573139190674\n",
            "EPOCH: 0 STEP: 400 VAL_LOSS: 2.1019225120544434\n",
            "EPOCH: 0 STEP: 425 VAL_LOSS: 2.0594897270202637\n",
            "EPOCH: 0 STEP: 450 VAL_LOSS: 2.044118881225586\n",
            "EPOCH: 0 STEP: 475 VAL_LOSS: 1.9981765747070312\n",
            "EPOCH: 1 STEP: 500 VAL_LOSS: 1.9742612838745117\n",
            "EPOCH: 1 STEP: 525 VAL_LOSS: 1.9453418254852295\n",
            "EPOCH: 1 STEP: 550 VAL_LOSS: 1.917400598526001\n",
            "EPOCH: 1 STEP: 575 VAL_LOSS: 1.89961576461792\n",
            "EPOCH: 1 STEP: 600 VAL_LOSS: 1.8748400211334229\n",
            "EPOCH: 1 STEP: 625 VAL_LOSS: 1.858496904373169\n",
            "EPOCH: 1 STEP: 650 VAL_LOSS: 1.8331668376922607\n",
            "EPOCH: 1 STEP: 675 VAL_LOSS: 1.817090392112732\n",
            "EPOCH: 1 STEP: 700 VAL_LOSS: 1.798350214958191\n",
            "EPOCH: 1 STEP: 725 VAL_LOSS: 1.7801456451416016\n",
            "EPOCH: 1 STEP: 750 VAL_LOSS: 1.7686976194381714\n",
            "EPOCH: 1 STEP: 775 VAL_LOSS: 1.7529375553131104\n",
            "EPOCH: 1 STEP: 800 VAL_LOSS: 1.7380300760269165\n",
            "EPOCH: 1 STEP: 825 VAL_LOSS: 1.7274062633514404\n",
            "EPOCH: 1 STEP: 850 VAL_LOSS: 1.7135822772979736\n",
            "EPOCH: 1 STEP: 875 VAL_LOSS: 1.7042162418365479\n",
            "EPOCH: 1 STEP: 900 VAL_LOSS: 1.696423053741455\n",
            "EPOCH: 1 STEP: 925 VAL_LOSS: 1.6816835403442383\n",
            "EPOCH: 1 STEP: 950 VAL_LOSS: 1.6715009212493896\n",
            "EPOCH: 1 STEP: 975 VAL_LOSS: 1.6569222211837769\n",
            "EPOCH: 2 STEP: 1000 VAL_LOSS: 1.6554670333862305\n",
            "EPOCH: 2 STEP: 1025 VAL_LOSS: 1.6469497680664062\n",
            "EPOCH: 2 STEP: 1050 VAL_LOSS: 1.6358470916748047\n",
            "EPOCH: 2 STEP: 1075 VAL_LOSS: 1.6280508041381836\n",
            "EPOCH: 2 STEP: 1100 VAL_LOSS: 1.6184494495391846\n",
            "EPOCH: 2 STEP: 1125 VAL_LOSS: 1.6107165813446045\n",
            "EPOCH: 2 STEP: 1150 VAL_LOSS: 1.6035923957824707\n",
            "EPOCH: 2 STEP: 1175 VAL_LOSS: 1.5919256210327148\n",
            "EPOCH: 2 STEP: 1200 VAL_LOSS: 1.581142544746399\n",
            "EPOCH: 2 STEP: 1225 VAL_LOSS: 1.576517939567566\n",
            "EPOCH: 2 STEP: 1250 VAL_LOSS: 1.5723822116851807\n",
            "EPOCH: 2 STEP: 1275 VAL_LOSS: 1.5690791606903076\n",
            "EPOCH: 2 STEP: 1300 VAL_LOSS: 1.5612730979919434\n",
            "EPOCH: 2 STEP: 1325 VAL_LOSS: 1.5608032941818237\n",
            "EPOCH: 2 STEP: 1350 VAL_LOSS: 1.5511391162872314\n",
            "EPOCH: 2 STEP: 1375 VAL_LOSS: 1.547182321548462\n",
            "EPOCH: 2 STEP: 1400 VAL_LOSS: 1.5445685386657715\n",
            "EPOCH: 2 STEP: 1425 VAL_LOSS: 1.5402721166610718\n",
            "EPOCH: 2 STEP: 1450 VAL_LOSS: 1.5320805311203003\n",
            "EPOCH: 3 STEP: 1475 VAL_LOSS: 1.5337307453155518\n",
            "EPOCH: 3 STEP: 1500 VAL_LOSS: 1.5211546421051025\n",
            "EPOCH: 3 STEP: 1525 VAL_LOSS: 1.5140271186828613\n",
            "EPOCH: 3 STEP: 1550 VAL_LOSS: 1.513713002204895\n",
            "EPOCH: 3 STEP: 1575 VAL_LOSS: 1.5100127458572388\n",
            "EPOCH: 3 STEP: 1600 VAL_LOSS: 1.5044467449188232\n",
            "EPOCH: 3 STEP: 1625 VAL_LOSS: 1.4978562593460083\n",
            "EPOCH: 3 STEP: 1650 VAL_LOSS: 1.4944450855255127\n",
            "EPOCH: 3 STEP: 1675 VAL_LOSS: 1.4953522682189941\n",
            "EPOCH: 3 STEP: 1700 VAL_LOSS: 1.4893230199813843\n",
            "EPOCH: 3 STEP: 1725 VAL_LOSS: 1.484495759010315\n",
            "EPOCH: 3 STEP: 1750 VAL_LOSS: 1.4862923622131348\n",
            "EPOCH: 3 STEP: 1775 VAL_LOSS: 1.4799737930297852\n",
            "EPOCH: 3 STEP: 1800 VAL_LOSS: 1.4791089296340942\n",
            "EPOCH: 3 STEP: 1825 VAL_LOSS: 1.4733288288116455\n",
            "EPOCH: 3 STEP: 1850 VAL_LOSS: 1.4727822542190552\n",
            "EPOCH: 3 STEP: 1875 VAL_LOSS: 1.4746814966201782\n",
            "EPOCH: 3 STEP: 1900 VAL_LOSS: 1.4679381847381592\n",
            "EPOCH: 3 STEP: 1925 VAL_LOSS: 1.4625978469848633\n",
            "EPOCH: 3 STEP: 1950 VAL_LOSS: 1.460745930671692\n",
            "EPOCH: 4 STEP: 1975 VAL_LOSS: 1.457861065864563\n",
            "EPOCH: 4 STEP: 2000 VAL_LOSS: 1.4574910402297974\n",
            "EPOCH: 4 STEP: 2025 VAL_LOSS: 1.451141595840454\n",
            "EPOCH: 4 STEP: 2050 VAL_LOSS: 1.4482877254486084\n",
            "EPOCH: 4 STEP: 2075 VAL_LOSS: 1.4498764276504517\n",
            "EPOCH: 4 STEP: 2100 VAL_LOSS: 1.4439408779144287\n",
            "EPOCH: 4 STEP: 2125 VAL_LOSS: 1.4403560161590576\n",
            "EPOCH: 4 STEP: 2150 VAL_LOSS: 1.4342119693756104\n",
            "EPOCH: 4 STEP: 2175 VAL_LOSS: 1.4322971105575562\n",
            "EPOCH: 4 STEP: 2200 VAL_LOSS: 1.4319560527801514\n",
            "EPOCH: 4 STEP: 2225 VAL_LOSS: 1.430039644241333\n",
            "EPOCH: 4 STEP: 2250 VAL_LOSS: 1.4300075769424438\n",
            "EPOCH: 4 STEP: 2275 VAL_LOSS: 1.4307760000228882\n",
            "EPOCH: 4 STEP: 2300 VAL_LOSS: 1.4304018020629883\n",
            "EPOCH: 4 STEP: 2325 VAL_LOSS: 1.4317554235458374\n",
            "EPOCH: 4 STEP: 2350 VAL_LOSS: 1.4297897815704346\n",
            "EPOCH: 4 STEP: 2375 VAL_LOSS: 1.426754355430603\n",
            "EPOCH: 4 STEP: 2400 VAL_LOSS: 1.4239684343338013\n",
            "EPOCH: 4 STEP: 2425 VAL_LOSS: 1.414427399635315\n",
            "EPOCH: 4 STEP: 2450 VAL_LOSS: 1.4120086431503296\n",
            "EPOCH: 5 STEP: 2475 VAL_LOSS: 1.4104838371276855\n",
            "EPOCH: 5 STEP: 2500 VAL_LOSS: 1.4153472185134888\n",
            "EPOCH: 5 STEP: 2525 VAL_LOSS: 1.4080370664596558\n",
            "EPOCH: 5 STEP: 2550 VAL_LOSS: 1.412744164466858\n",
            "EPOCH: 5 STEP: 2575 VAL_LOSS: 1.4075233936309814\n",
            "EPOCH: 5 STEP: 2600 VAL_LOSS: 1.4057862758636475\n",
            "EPOCH: 5 STEP: 2625 VAL_LOSS: 1.4005768299102783\n",
            "EPOCH: 5 STEP: 2650 VAL_LOSS: 1.3926899433135986\n",
            "EPOCH: 5 STEP: 2675 VAL_LOSS: 1.3926291465759277\n",
            "EPOCH: 5 STEP: 2700 VAL_LOSS: 1.3932454586029053\n",
            "EPOCH: 5 STEP: 2725 VAL_LOSS: 1.396817684173584\n",
            "EPOCH: 5 STEP: 2750 VAL_LOSS: 1.3902907371520996\n",
            "EPOCH: 5 STEP: 2775 VAL_LOSS: 1.392331600189209\n",
            "EPOCH: 5 STEP: 2800 VAL_LOSS: 1.3985772132873535\n",
            "EPOCH: 5 STEP: 2825 VAL_LOSS: 1.3927017450332642\n",
            "EPOCH: 5 STEP: 2850 VAL_LOSS: 1.3975101709365845\n",
            "EPOCH: 5 STEP: 2875 VAL_LOSS: 1.396628975868225\n",
            "EPOCH: 5 STEP: 2900 VAL_LOSS: 1.3888344764709473\n",
            "EPOCH: 5 STEP: 2925 VAL_LOSS: 1.3838350772857666\n",
            "EPOCH: 6 STEP: 2950 VAL_LOSS: 1.3808921575546265\n",
            "EPOCH: 6 STEP: 2975 VAL_LOSS: 1.3804476261138916\n",
            "EPOCH: 6 STEP: 3000 VAL_LOSS: 1.3790881633758545\n",
            "EPOCH: 6 STEP: 3025 VAL_LOSS: 1.380239486694336\n",
            "EPOCH: 6 STEP: 3050 VAL_LOSS: 1.3823518753051758\n",
            "EPOCH: 6 STEP: 3075 VAL_LOSS: 1.3787553310394287\n",
            "EPOCH: 6 STEP: 3100 VAL_LOSS: 1.3808085918426514\n",
            "EPOCH: 6 STEP: 3125 VAL_LOSS: 1.373241901397705\n",
            "EPOCH: 6 STEP: 3150 VAL_LOSS: 1.369571566581726\n",
            "EPOCH: 6 STEP: 3175 VAL_LOSS: 1.3735406398773193\n",
            "EPOCH: 6 STEP: 3200 VAL_LOSS: 1.372971534729004\n",
            "EPOCH: 6 STEP: 3225 VAL_LOSS: 1.3722169399261475\n",
            "EPOCH: 6 STEP: 3250 VAL_LOSS: 1.3714332580566406\n",
            "EPOCH: 6 STEP: 3275 VAL_LOSS: 1.3763794898986816\n",
            "EPOCH: 6 STEP: 3300 VAL_LOSS: 1.3755807876586914\n",
            "EPOCH: 6 STEP: 3325 VAL_LOSS: 1.3811067342758179\n",
            "EPOCH: 6 STEP: 3350 VAL_LOSS: 1.3800787925720215\n",
            "EPOCH: 6 STEP: 3375 VAL_LOSS: 1.366803765296936\n",
            "EPOCH: 6 STEP: 3400 VAL_LOSS: 1.3726656436920166\n",
            "EPOCH: 6 STEP: 3425 VAL_LOSS: 1.3636068105697632\n",
            "EPOCH: 7 STEP: 3450 VAL_LOSS: 1.3636568784713745\n",
            "EPOCH: 7 STEP: 3475 VAL_LOSS: 1.367876410484314\n",
            "EPOCH: 7 STEP: 3500 VAL_LOSS: 1.3626140356063843\n",
            "EPOCH: 7 STEP: 3525 VAL_LOSS: 1.3610478639602661\n",
            "EPOCH: 7 STEP: 3550 VAL_LOSS: 1.3625104427337646\n",
            "EPOCH: 7 STEP: 3575 VAL_LOSS: 1.3658965826034546\n",
            "EPOCH: 7 STEP: 3600 VAL_LOSS: 1.3639004230499268\n",
            "EPOCH: 7 STEP: 3625 VAL_LOSS: 1.3580138683319092\n",
            "EPOCH: 7 STEP: 3650 VAL_LOSS: 1.3548986911773682\n",
            "EPOCH: 7 STEP: 3675 VAL_LOSS: 1.356622338294983\n",
            "EPOCH: 7 STEP: 3700 VAL_LOSS: 1.3533393144607544\n",
            "EPOCH: 7 STEP: 3725 VAL_LOSS: 1.3505831956863403\n",
            "EPOCH: 7 STEP: 3750 VAL_LOSS: 1.35397207736969\n",
            "EPOCH: 7 STEP: 3775 VAL_LOSS: 1.357432246208191\n",
            "EPOCH: 7 STEP: 3800 VAL_LOSS: 1.355925440788269\n",
            "EPOCH: 7 STEP: 3825 VAL_LOSS: 1.3580377101898193\n",
            "EPOCH: 7 STEP: 3850 VAL_LOSS: 1.359649419784546\n",
            "EPOCH: 7 STEP: 3875 VAL_LOSS: 1.3604494333267212\n",
            "EPOCH: 7 STEP: 3900 VAL_LOSS: 1.3590319156646729\n",
            "EPOCH: 8 STEP: 3925 VAL_LOSS: 1.3597708940505981\n",
            "EPOCH: 8 STEP: 3950 VAL_LOSS: 1.3560434579849243\n",
            "EPOCH: 8 STEP: 3975 VAL_LOSS: 1.352126121520996\n",
            "EPOCH: 8 STEP: 4000 VAL_LOSS: 1.3572900295257568\n",
            "EPOCH: 8 STEP: 4025 VAL_LOSS: 1.354051113128662\n",
            "EPOCH: 8 STEP: 4050 VAL_LOSS: 1.3504841327667236\n",
            "EPOCH: 8 STEP: 4075 VAL_LOSS: 1.3515082597732544\n",
            "EPOCH: 8 STEP: 4100 VAL_LOSS: 1.3508466482162476\n",
            "EPOCH: 8 STEP: 4125 VAL_LOSS: 1.3486344814300537\n",
            "EPOCH: 8 STEP: 4150 VAL_LOSS: 1.3429588079452515\n",
            "EPOCH: 8 STEP: 4175 VAL_LOSS: 1.345909595489502\n",
            "EPOCH: 8 STEP: 4200 VAL_LOSS: 1.3467328548431396\n",
            "EPOCH: 8 STEP: 4225 VAL_LOSS: 1.3480432033538818\n",
            "EPOCH: 8 STEP: 4250 VAL_LOSS: 1.3515056371688843\n",
            "EPOCH: 8 STEP: 4275 VAL_LOSS: 1.3544267416000366\n",
            "EPOCH: 8 STEP: 4300 VAL_LOSS: 1.3436988592147827\n",
            "EPOCH: 8 STEP: 4325 VAL_LOSS: 1.353116512298584\n",
            "EPOCH: 8 STEP: 4350 VAL_LOSS: 1.348807692527771\n",
            "EPOCH: 8 STEP: 4375 VAL_LOSS: 1.3481900691986084\n",
            "EPOCH: 8 STEP: 4400 VAL_LOSS: 1.3491737842559814\n",
            "EPOCH: 9 STEP: 4425 VAL_LOSS: 1.3469051122665405\n",
            "EPOCH: 9 STEP: 4450 VAL_LOSS: 1.3438302278518677\n",
            "EPOCH: 9 STEP: 4475 VAL_LOSS: 1.3456289768218994\n",
            "EPOCH: 9 STEP: 4500 VAL_LOSS: 1.3400120735168457\n",
            "EPOCH: 9 STEP: 4525 VAL_LOSS: 1.3449925184249878\n",
            "EPOCH: 9 STEP: 4550 VAL_LOSS: 1.3457741737365723\n",
            "EPOCH: 9 STEP: 4575 VAL_LOSS: 1.344010353088379\n",
            "EPOCH: 9 STEP: 4600 VAL_LOSS: 1.3405873775482178\n",
            "EPOCH: 9 STEP: 4625 VAL_LOSS: 1.3410446643829346\n",
            "EPOCH: 9 STEP: 4650 VAL_LOSS: 1.3370283842086792\n",
            "EPOCH: 9 STEP: 4675 VAL_LOSS: 1.3345786333084106\n",
            "EPOCH: 9 STEP: 4700 VAL_LOSS: 1.337925672531128\n",
            "EPOCH: 9 STEP: 4725 VAL_LOSS: 1.3406296968460083\n",
            "EPOCH: 9 STEP: 4750 VAL_LOSS: 1.3389346599578857\n",
            "EPOCH: 9 STEP: 4775 VAL_LOSS: 1.3415589332580566\n",
            "EPOCH: 9 STEP: 4800 VAL_LOSS: 1.3422518968582153\n",
            "EPOCH: 9 STEP: 4825 VAL_LOSS: 1.3435314893722534\n",
            "EPOCH: 9 STEP: 4850 VAL_LOSS: 1.3426240682601929\n",
            "EPOCH: 9 STEP: 4875 VAL_LOSS: 1.339410424232483\n",
            "EPOCH: 9 STEP: 4900 VAL_LOSS: 1.3370931148529053\n",
            "EPOCH: 10 STEP: 4925 VAL_LOSS: 1.3336492776870728\n",
            "EPOCH: 10 STEP: 4950 VAL_LOSS: 1.3359521627426147\n",
            "EPOCH: 10 STEP: 4975 VAL_LOSS: 1.3321369886398315\n",
            "EPOCH: 10 STEP: 5000 VAL_LOSS: 1.3335928916931152\n",
            "EPOCH: 10 STEP: 5025 VAL_LOSS: 1.3357213735580444\n",
            "EPOCH: 10 STEP: 5050 VAL_LOSS: 1.3360248804092407\n",
            "EPOCH: 10 STEP: 5075 VAL_LOSS: 1.3342276811599731\n",
            "EPOCH: 10 STEP: 5100 VAL_LOSS: 1.3302655220031738\n",
            "EPOCH: 10 STEP: 5125 VAL_LOSS: 1.3318928480148315\n",
            "EPOCH: 10 STEP: 5150 VAL_LOSS: 1.3337410688400269\n",
            "EPOCH: 10 STEP: 5175 VAL_LOSS: 1.33504319190979\n",
            "EPOCH: 10 STEP: 5200 VAL_LOSS: 1.3360788822174072\n",
            "EPOCH: 10 STEP: 5225 VAL_LOSS: 1.3323383331298828\n",
            "EPOCH: 10 STEP: 5250 VAL_LOSS: 1.3345270156860352\n",
            "EPOCH: 10 STEP: 5275 VAL_LOSS: 1.3290776014328003\n",
            "EPOCH: 10 STEP: 5300 VAL_LOSS: 1.3342159986495972\n",
            "EPOCH: 10 STEP: 5325 VAL_LOSS: 1.3416821956634521\n",
            "EPOCH: 10 STEP: 5350 VAL_LOSS: 1.334139108657837\n",
            "EPOCH: 10 STEP: 5375 VAL_LOSS: 1.3370448350906372\n",
            "EPOCH: 11 STEP: 5400 VAL_LOSS: 1.3334811925888062\n",
            "EPOCH: 11 STEP: 5425 VAL_LOSS: 1.3310542106628418\n",
            "EPOCH: 11 STEP: 5450 VAL_LOSS: 1.3366303443908691\n",
            "EPOCH: 11 STEP: 5475 VAL_LOSS: 1.3328100442886353\n",
            "EPOCH: 11 STEP: 5500 VAL_LOSS: 1.3309040069580078\n",
            "EPOCH: 11 STEP: 5525 VAL_LOSS: 1.3254257440567017\n",
            "EPOCH: 11 STEP: 5550 VAL_LOSS: 1.3341948986053467\n",
            "EPOCH: 11 STEP: 5575 VAL_LOSS: 1.328991174697876\n",
            "EPOCH: 11 STEP: 5600 VAL_LOSS: 1.3289680480957031\n",
            "EPOCH: 11 STEP: 5625 VAL_LOSS: 1.3252092599868774\n",
            "EPOCH: 11 STEP: 5650 VAL_LOSS: 1.3274792432785034\n",
            "EPOCH: 11 STEP: 5675 VAL_LOSS: 1.326930046081543\n",
            "EPOCH: 11 STEP: 5700 VAL_LOSS: 1.3229442834854126\n",
            "EPOCH: 11 STEP: 5725 VAL_LOSS: 1.3274701833724976\n",
            "EPOCH: 11 STEP: 5750 VAL_LOSS: 1.3269292116165161\n",
            "EPOCH: 11 STEP: 5775 VAL_LOSS: 1.3257936239242554\n",
            "EPOCH: 11 STEP: 5800 VAL_LOSS: 1.3331536054611206\n",
            "EPOCH: 11 STEP: 5825 VAL_LOSS: 1.3336490392684937\n",
            "EPOCH: 11 STEP: 5850 VAL_LOSS: 1.3339051008224487\n",
            "EPOCH: 11 STEP: 5875 VAL_LOSS: 1.3292198181152344\n",
            "EPOCH: 12 STEP: 5900 VAL_LOSS: 1.3293089866638184\n",
            "EPOCH: 12 STEP: 5925 VAL_LOSS: 1.3294075727462769\n",
            "EPOCH: 12 STEP: 5950 VAL_LOSS: 1.3285070657730103\n",
            "EPOCH: 12 STEP: 5975 VAL_LOSS: 1.3219517469406128\n",
            "EPOCH: 12 STEP: 6000 VAL_LOSS: 1.3284239768981934\n",
            "EPOCH: 12 STEP: 6025 VAL_LOSS: 1.32925546169281\n",
            "EPOCH: 12 STEP: 6050 VAL_LOSS: 1.3271857500076294\n",
            "EPOCH: 12 STEP: 6075 VAL_LOSS: 1.3264820575714111\n",
            "EPOCH: 12 STEP: 6100 VAL_LOSS: 1.3213205337524414\n",
            "EPOCH: 12 STEP: 6125 VAL_LOSS: 1.3236870765686035\n",
            "EPOCH: 12 STEP: 6150 VAL_LOSS: 1.3188611268997192\n",
            "EPOCH: 12 STEP: 6175 VAL_LOSS: 1.3229421377182007\n",
            "EPOCH: 12 STEP: 6200 VAL_LOSS: 1.3210668563842773\n",
            "EPOCH: 12 STEP: 6225 VAL_LOSS: 1.3289114236831665\n",
            "EPOCH: 12 STEP: 6250 VAL_LOSS: 1.3245638608932495\n",
            "EPOCH: 12 STEP: 6275 VAL_LOSS: 1.3264607191085815\n",
            "EPOCH: 12 STEP: 6300 VAL_LOSS: 1.330505132675171\n",
            "EPOCH: 12 STEP: 6325 VAL_LOSS: 1.324890375137329\n",
            "EPOCH: 12 STEP: 6350 VAL_LOSS: 1.3219456672668457\n",
            "EPOCH: 13 STEP: 6375 VAL_LOSS: 1.3261419534683228\n",
            "EPOCH: 13 STEP: 6400 VAL_LOSS: 1.321475625038147\n",
            "EPOCH: 13 STEP: 6425 VAL_LOSS: 1.3239154815673828\n",
            "EPOCH: 13 STEP: 6450 VAL_LOSS: 1.3285465240478516\n",
            "EPOCH: 13 STEP: 6475 VAL_LOSS: 1.3229950666427612\n",
            "EPOCH: 13 STEP: 6500 VAL_LOSS: 1.3254210948944092\n",
            "EPOCH: 13 STEP: 6525 VAL_LOSS: 1.33345365524292\n",
            "EPOCH: 13 STEP: 6550 VAL_LOSS: 1.3280047178268433\n",
            "EPOCH: 13 STEP: 6575 VAL_LOSS: 1.3284944295883179\n",
            "EPOCH: 13 STEP: 6600 VAL_LOSS: 1.3206080198287964\n",
            "EPOCH: 13 STEP: 6625 VAL_LOSS: 1.32826566696167\n",
            "EPOCH: 13 STEP: 6650 VAL_LOSS: 1.3201053142547607\n",
            "EPOCH: 13 STEP: 6675 VAL_LOSS: 1.320682168006897\n",
            "EPOCH: 13 STEP: 6700 VAL_LOSS: 1.32047700881958\n",
            "EPOCH: 13 STEP: 6725 VAL_LOSS: 1.3258436918258667\n",
            "EPOCH: 13 STEP: 6750 VAL_LOSS: 1.320495367050171\n",
            "EPOCH: 13 STEP: 6775 VAL_LOSS: 1.3251911401748657\n",
            "EPOCH: 13 STEP: 6800 VAL_LOSS: 1.3326573371887207\n",
            "EPOCH: 13 STEP: 6825 VAL_LOSS: 1.3304245471954346\n",
            "EPOCH: 13 STEP: 6850 VAL_LOSS: 1.3247272968292236\n",
            "EPOCH: 14 STEP: 6875 VAL_LOSS: 1.3249448537826538\n",
            "EPOCH: 14 STEP: 6900 VAL_LOSS: 1.3240407705307007\n",
            "EPOCH: 14 STEP: 6925 VAL_LOSS: 1.3217278718948364\n",
            "EPOCH: 14 STEP: 6950 VAL_LOSS: 1.3235334157943726\n",
            "EPOCH: 14 STEP: 6975 VAL_LOSS: 1.3258488178253174\n",
            "EPOCH: 14 STEP: 7000 VAL_LOSS: 1.32450532913208\n",
            "EPOCH: 14 STEP: 7025 VAL_LOSS: 1.3263120651245117\n",
            "EPOCH: 14 STEP: 7050 VAL_LOSS: 1.3212969303131104\n",
            "EPOCH: 14 STEP: 7075 VAL_LOSS: 1.3231109380722046\n",
            "EPOCH: 14 STEP: 7100 VAL_LOSS: 1.3186089992523193\n",
            "EPOCH: 14 STEP: 7125 VAL_LOSS: 1.312438726425171\n",
            "EPOCH: 14 STEP: 7150 VAL_LOSS: 1.3189040422439575\n",
            "EPOCH: 14 STEP: 7175 VAL_LOSS: 1.3162702322006226\n",
            "EPOCH: 14 STEP: 7200 VAL_LOSS: 1.3179712295532227\n",
            "EPOCH: 14 STEP: 7225 VAL_LOSS: 1.318364143371582\n",
            "EPOCH: 14 STEP: 7250 VAL_LOSS: 1.3145620822906494\n",
            "EPOCH: 14 STEP: 7275 VAL_LOSS: 1.321118712425232\n",
            "EPOCH: 14 STEP: 7300 VAL_LOSS: 1.3148785829544067\n",
            "EPOCH: 14 STEP: 7325 VAL_LOSS: 1.3217717409133911\n",
            "EPOCH: 14 STEP: 7350 VAL_LOSS: 1.3195356130599976\n",
            "EPOCH: 15 STEP: 7375 VAL_LOSS: 1.3253620862960815\n",
            "EPOCH: 15 STEP: 7400 VAL_LOSS: 1.3239482641220093\n",
            "EPOCH: 15 STEP: 7425 VAL_LOSS: 1.3233003616333008\n",
            "EPOCH: 15 STEP: 7450 VAL_LOSS: 1.322912335395813\n",
            "EPOCH: 15 STEP: 7475 VAL_LOSS: 1.3221795558929443\n",
            "EPOCH: 15 STEP: 7500 VAL_LOSS: 1.3203097581863403\n",
            "EPOCH: 15 STEP: 7525 VAL_LOSS: 1.3255354166030884\n",
            "EPOCH: 15 STEP: 7550 VAL_LOSS: 1.3218728303909302\n",
            "EPOCH: 15 STEP: 7575 VAL_LOSS: 1.3216233253479004\n",
            "EPOCH: 15 STEP: 7600 VAL_LOSS: 1.3195445537567139\n",
            "EPOCH: 15 STEP: 7625 VAL_LOSS: 1.3227298259735107\n",
            "EPOCH: 15 STEP: 7650 VAL_LOSS: 1.3185100555419922\n",
            "EPOCH: 15 STEP: 7675 VAL_LOSS: 1.314609408378601\n",
            "EPOCH: 15 STEP: 7700 VAL_LOSS: 1.3203190565109253\n",
            "EPOCH: 15 STEP: 7725 VAL_LOSS: 1.3194210529327393\n",
            "EPOCH: 15 STEP: 7750 VAL_LOSS: 1.3200056552886963\n",
            "EPOCH: 15 STEP: 7775 VAL_LOSS: 1.3211379051208496\n",
            "EPOCH: 15 STEP: 7800 VAL_LOSS: 1.3213303089141846\n",
            "EPOCH: 15 STEP: 7825 VAL_LOSS: 1.3201148509979248\n",
            "EPOCH: 16 STEP: 7850 VAL_LOSS: 1.3196879625320435\n",
            "EPOCH: 16 STEP: 7875 VAL_LOSS: 1.3174352645874023\n",
            "EPOCH: 16 STEP: 7900 VAL_LOSS: 1.3199243545532227\n",
            "EPOCH: 16 STEP: 7925 VAL_LOSS: 1.3152941465377808\n",
            "EPOCH: 16 STEP: 7950 VAL_LOSS: 1.321518898010254\n",
            "EPOCH: 16 STEP: 7975 VAL_LOSS: 1.3137621879577637\n",
            "EPOCH: 16 STEP: 8000 VAL_LOSS: 1.317901611328125\n",
            "EPOCH: 16 STEP: 8025 VAL_LOSS: 1.3114887475967407\n",
            "EPOCH: 16 STEP: 8050 VAL_LOSS: 1.3138182163238525\n",
            "EPOCH: 16 STEP: 8075 VAL_LOSS: 1.3147242069244385\n",
            "EPOCH: 16 STEP: 8100 VAL_LOSS: 1.3163622617721558\n",
            "EPOCH: 16 STEP: 8125 VAL_LOSS: 1.3110548257827759\n",
            "EPOCH: 16 STEP: 8150 VAL_LOSS: 1.3102947473526\n",
            "EPOCH: 16 STEP: 8175 VAL_LOSS: 1.3111234903335571\n",
            "EPOCH: 16 STEP: 8200 VAL_LOSS: 1.3158279657363892\n",
            "EPOCH: 16 STEP: 8225 VAL_LOSS: 1.3201160430908203\n",
            "EPOCH: 16 STEP: 8250 VAL_LOSS: 1.3182021379470825\n",
            "EPOCH: 16 STEP: 8275 VAL_LOSS: 1.3153879642486572\n",
            "EPOCH: 16 STEP: 8300 VAL_LOSS: 1.3211981058120728\n",
            "EPOCH: 16 STEP: 8325 VAL_LOSS: 1.3157199621200562\n",
            "EPOCH: 17 STEP: 8350 VAL_LOSS: 1.3155648708343506\n",
            "EPOCH: 17 STEP: 8375 VAL_LOSS: 1.3206802606582642\n",
            "EPOCH: 17 STEP: 8400 VAL_LOSS: 1.3151054382324219\n",
            "EPOCH: 17 STEP: 8425 VAL_LOSS: 1.3159544467926025\n",
            "EPOCH: 17 STEP: 8450 VAL_LOSS: 1.3141201734542847\n",
            "EPOCH: 17 STEP: 8475 VAL_LOSS: 1.3152979612350464\n",
            "EPOCH: 17 STEP: 8500 VAL_LOSS: 1.311566710472107\n",
            "EPOCH: 17 STEP: 8525 VAL_LOSS: 1.3137606382369995\n",
            "EPOCH: 17 STEP: 8550 VAL_LOSS: 1.3138209581375122\n",
            "EPOCH: 17 STEP: 8575 VAL_LOSS: 1.3056137561798096\n",
            "EPOCH: 17 STEP: 8600 VAL_LOSS: 1.3131836652755737\n",
            "EPOCH: 17 STEP: 8625 VAL_LOSS: 1.3144007921218872\n",
            "EPOCH: 17 STEP: 8650 VAL_LOSS: 1.3069645166397095\n",
            "EPOCH: 17 STEP: 8675 VAL_LOSS: 1.313850998878479\n",
            "EPOCH: 17 STEP: 8700 VAL_LOSS: 1.3110681772232056\n",
            "EPOCH: 17 STEP: 8725 VAL_LOSS: 1.3204458951950073\n",
            "EPOCH: 17 STEP: 8750 VAL_LOSS: 1.3218441009521484\n",
            "EPOCH: 17 STEP: 8775 VAL_LOSS: 1.31802499294281\n",
            "EPOCH: 17 STEP: 8800 VAL_LOSS: 1.3155944347381592\n",
            "EPOCH: 18 STEP: 8825 VAL_LOSS: 1.3195732831954956\n",
            "EPOCH: 18 STEP: 8850 VAL_LOSS: 1.3162106275558472\n",
            "EPOCH: 18 STEP: 8875 VAL_LOSS: 1.3174803256988525\n",
            "EPOCH: 18 STEP: 8900 VAL_LOSS: 1.3130030632019043\n",
            "EPOCH: 18 STEP: 8925 VAL_LOSS: 1.3131541013717651\n",
            "EPOCH: 18 STEP: 8950 VAL_LOSS: 1.310310959815979\n",
            "EPOCH: 18 STEP: 8975 VAL_LOSS: 1.3136756420135498\n",
            "EPOCH: 18 STEP: 9000 VAL_LOSS: 1.3183132410049438\n",
            "EPOCH: 18 STEP: 9025 VAL_LOSS: 1.3123247623443604\n",
            "EPOCH: 18 STEP: 9050 VAL_LOSS: 1.3099652528762817\n",
            "EPOCH: 18 STEP: 9075 VAL_LOSS: 1.3125425577163696\n",
            "EPOCH: 18 STEP: 9100 VAL_LOSS: 1.3099033832550049\n",
            "EPOCH: 18 STEP: 9125 VAL_LOSS: 1.3118771314620972\n",
            "EPOCH: 18 STEP: 9150 VAL_LOSS: 1.3113754987716675\n",
            "EPOCH: 18 STEP: 9175 VAL_LOSS: 1.31882905960083\n",
            "EPOCH: 18 STEP: 9200 VAL_LOSS: 1.3129768371582031\n",
            "EPOCH: 18 STEP: 9225 VAL_LOSS: 1.3155689239501953\n",
            "EPOCH: 18 STEP: 9250 VAL_LOSS: 1.323166012763977\n",
            "EPOCH: 18 STEP: 9275 VAL_LOSS: 1.3133771419525146\n",
            "EPOCH: 18 STEP: 9300 VAL_LOSS: 1.3115800619125366\n",
            "EPOCH: 19 STEP: 9325 VAL_LOSS: 1.315855622291565\n",
            "EPOCH: 19 STEP: 9350 VAL_LOSS: 1.3130707740783691\n",
            "EPOCH: 19 STEP: 9375 VAL_LOSS: 1.3128713369369507\n",
            "EPOCH: 19 STEP: 9400 VAL_LOSS: 1.309926986694336\n",
            "EPOCH: 19 STEP: 9425 VAL_LOSS: 1.3181504011154175\n",
            "EPOCH: 19 STEP: 9450 VAL_LOSS: 1.3214291334152222\n",
            "EPOCH: 19 STEP: 9475 VAL_LOSS: 1.315745234489441\n",
            "EPOCH: 19 STEP: 9500 VAL_LOSS: 1.3111213445663452\n",
            "EPOCH: 19 STEP: 9525 VAL_LOSS: 1.3107720613479614\n",
            "EPOCH: 19 STEP: 9550 VAL_LOSS: 1.3102550506591797\n",
            "EPOCH: 19 STEP: 9575 VAL_LOSS: 1.3101778030395508\n",
            "EPOCH: 19 STEP: 9600 VAL_LOSS: 1.3091042041778564\n",
            "EPOCH: 19 STEP: 9625 VAL_LOSS: 1.306990385055542\n",
            "EPOCH: 19 STEP: 9650 VAL_LOSS: 1.313075065612793\n",
            "EPOCH: 19 STEP: 9675 VAL_LOSS: 1.3084982633590698\n",
            "EPOCH: 19 STEP: 9700 VAL_LOSS: 1.312705636024475\n",
            "EPOCH: 19 STEP: 9725 VAL_LOSS: 1.3183164596557617\n",
            "EPOCH: 19 STEP: 9750 VAL_LOSS: 1.3127557039260864\n",
            "EPOCH: 19 STEP: 9775 VAL_LOSS: 1.3143807649612427\n",
            "EPOCH: 19 STEP: 9800 VAL_LOSS: 1.3109493255615234\n",
            "EPOCH: 20 STEP: 9825 VAL_LOSS: 1.3161379098892212\n",
            "EPOCH: 20 STEP: 9850 VAL_LOSS: 1.3076515197753906\n",
            "EPOCH: 20 STEP: 9875 VAL_LOSS: 1.3134756088256836\n",
            "EPOCH: 20 STEP: 9900 VAL_LOSS: 1.3127378225326538\n",
            "EPOCH: 20 STEP: 9925 VAL_LOSS: 1.3112620115280151\n",
            "EPOCH: 20 STEP: 9950 VAL_LOSS: 1.314479947090149\n",
            "EPOCH: 20 STEP: 9975 VAL_LOSS: 1.3087350130081177\n",
            "EPOCH: 20 STEP: 10000 VAL_LOSS: 1.3096269369125366\n",
            "EPOCH: 20 STEP: 10025 VAL_LOSS: 1.3076598644256592\n",
            "EPOCH: 20 STEP: 10050 VAL_LOSS: 1.3102086782455444\n",
            "EPOCH: 20 STEP: 10075 VAL_LOSS: 1.3107160329818726\n",
            "EPOCH: 20 STEP: 10100 VAL_LOSS: 1.3133556842803955\n",
            "EPOCH: 20 STEP: 10125 VAL_LOSS: 1.304294466972351\n",
            "EPOCH: 20 STEP: 10150 VAL_LOSS: 1.310494303703308\n",
            "EPOCH: 20 STEP: 10175 VAL_LOSS: 1.3083025217056274\n",
            "EPOCH: 20 STEP: 10200 VAL_LOSS: 1.307715892791748\n",
            "EPOCH: 20 STEP: 10225 VAL_LOSS: 1.3105180263519287\n",
            "EPOCH: 20 STEP: 10250 VAL_LOSS: 1.3080947399139404\n",
            "EPOCH: 20 STEP: 10275 VAL_LOSS: 1.3117692470550537\n",
            "EPOCH: 21 STEP: 10300 VAL_LOSS: 1.3121607303619385\n",
            "EPOCH: 21 STEP: 10325 VAL_LOSS: 1.3086066246032715\n",
            "EPOCH: 21 STEP: 10350 VAL_LOSS: 1.3135409355163574\n",
            "EPOCH: 21 STEP: 10375 VAL_LOSS: 1.3126291036605835\n",
            "EPOCH: 21 STEP: 10400 VAL_LOSS: 1.3126691579818726\n",
            "EPOCH: 21 STEP: 10425 VAL_LOSS: 1.308577299118042\n",
            "EPOCH: 21 STEP: 10450 VAL_LOSS: 1.3162931203842163\n",
            "EPOCH: 21 STEP: 10475 VAL_LOSS: 1.3096963167190552\n",
            "EPOCH: 21 STEP: 10500 VAL_LOSS: 1.3083325624465942\n",
            "EPOCH: 21 STEP: 10525 VAL_LOSS: 1.3079917430877686\n",
            "EPOCH: 21 STEP: 10550 VAL_LOSS: 1.307586908340454\n",
            "EPOCH: 21 STEP: 10575 VAL_LOSS: 1.3100676536560059\n",
            "EPOCH: 21 STEP: 10600 VAL_LOSS: 1.310817003250122\n",
            "EPOCH: 21 STEP: 10625 VAL_LOSS: 1.305794358253479\n",
            "EPOCH: 21 STEP: 10650 VAL_LOSS: 1.3095742464065552\n",
            "EPOCH: 21 STEP: 10675 VAL_LOSS: 1.307572364807129\n",
            "EPOCH: 21 STEP: 10700 VAL_LOSS: 1.3154155015945435\n",
            "EPOCH: 21 STEP: 10725 VAL_LOSS: 1.3070491552352905\n",
            "EPOCH: 21 STEP: 10750 VAL_LOSS: 1.3101381063461304\n",
            "EPOCH: 21 STEP: 10775 VAL_LOSS: 1.3083182573318481\n",
            "EPOCH: 22 STEP: 10800 VAL_LOSS: 1.3062233924865723\n",
            "EPOCH: 22 STEP: 10825 VAL_LOSS: 1.3139315843582153\n",
            "EPOCH: 22 STEP: 10850 VAL_LOSS: 1.308624505996704\n",
            "EPOCH: 22 STEP: 10875 VAL_LOSS: 1.3063441514968872\n",
            "EPOCH: 22 STEP: 10900 VAL_LOSS: 1.3092637062072754\n",
            "EPOCH: 22 STEP: 10925 VAL_LOSS: 1.3110026121139526\n",
            "EPOCH: 22 STEP: 10950 VAL_LOSS: 1.30980384349823\n",
            "EPOCH: 22 STEP: 10975 VAL_LOSS: 1.3114356994628906\n",
            "EPOCH: 22 STEP: 11000 VAL_LOSS: 1.312511920928955\n",
            "EPOCH: 22 STEP: 11025 VAL_LOSS: 1.3046451807022095\n",
            "EPOCH: 22 STEP: 11050 VAL_LOSS: 1.30780029296875\n",
            "EPOCH: 22 STEP: 11075 VAL_LOSS: 1.3098021745681763\n",
            "EPOCH: 22 STEP: 11100 VAL_LOSS: 1.303205132484436\n",
            "EPOCH: 22 STEP: 11125 VAL_LOSS: 1.3094781637191772\n",
            "EPOCH: 22 STEP: 11150 VAL_LOSS: 1.3116816282272339\n",
            "EPOCH: 22 STEP: 11175 VAL_LOSS: 1.317779541015625\n",
            "EPOCH: 22 STEP: 11200 VAL_LOSS: 1.3168039321899414\n",
            "EPOCH: 22 STEP: 11225 VAL_LOSS: 1.3128376007080078\n",
            "EPOCH: 22 STEP: 11250 VAL_LOSS: 1.3120317459106445\n",
            "EPOCH: 23 STEP: 11275 VAL_LOSS: 1.3171800374984741\n",
            "EPOCH: 23 STEP: 11300 VAL_LOSS: 1.3128935098648071\n",
            "EPOCH: 23 STEP: 11325 VAL_LOSS: 1.3135969638824463\n",
            "EPOCH: 23 STEP: 11350 VAL_LOSS: 1.3105875253677368\n",
            "EPOCH: 23 STEP: 11375 VAL_LOSS: 1.3106666803359985\n",
            "EPOCH: 23 STEP: 11400 VAL_LOSS: 1.3080743551254272\n",
            "EPOCH: 23 STEP: 11425 VAL_LOSS: 1.312190294265747\n",
            "EPOCH: 23 STEP: 11450 VAL_LOSS: 1.311700701713562\n",
            "EPOCH: 23 STEP: 11475 VAL_LOSS: 1.3060479164123535\n",
            "EPOCH: 23 STEP: 11500 VAL_LOSS: 1.3040711879730225\n",
            "EPOCH: 23 STEP: 11525 VAL_LOSS: 1.3064085245132446\n",
            "EPOCH: 23 STEP: 11550 VAL_LOSS: 1.3109997510910034\n",
            "EPOCH: 23 STEP: 11575 VAL_LOSS: 1.3050285577774048\n",
            "EPOCH: 23 STEP: 11600 VAL_LOSS: 1.3032615184783936\n",
            "EPOCH: 23 STEP: 11625 VAL_LOSS: 1.305113434791565\n",
            "EPOCH: 23 STEP: 11650 VAL_LOSS: 1.3055251836776733\n",
            "EPOCH: 23 STEP: 11675 VAL_LOSS: 1.3005354404449463\n",
            "EPOCH: 23 STEP: 11700 VAL_LOSS: 1.310478925704956\n",
            "EPOCH: 23 STEP: 11725 VAL_LOSS: 1.3032019138336182\n",
            "EPOCH: 23 STEP: 11750 VAL_LOSS: 1.3096219301223755\n",
            "EPOCH: 24 STEP: 11775 VAL_LOSS: 1.3067651987075806\n",
            "EPOCH: 24 STEP: 11800 VAL_LOSS: 1.308546543121338\n",
            "EPOCH: 24 STEP: 11825 VAL_LOSS: 1.3108662366867065\n",
            "EPOCH: 24 STEP: 11850 VAL_LOSS: 1.3038313388824463\n",
            "EPOCH: 24 STEP: 11875 VAL_LOSS: 1.3067370653152466\n",
            "EPOCH: 24 STEP: 11900 VAL_LOSS: 1.3125557899475098\n",
            "EPOCH: 24 STEP: 11925 VAL_LOSS: 1.30377995967865\n",
            "EPOCH: 24 STEP: 11950 VAL_LOSS: 1.3046889305114746\n",
            "EPOCH: 24 STEP: 11975 VAL_LOSS: 1.3076753616333008\n",
            "EPOCH: 24 STEP: 12000 VAL_LOSS: 1.3049211502075195\n",
            "EPOCH: 24 STEP: 12025 VAL_LOSS: 1.298644781112671\n",
            "EPOCH: 24 STEP: 12050 VAL_LOSS: 1.2992171049118042\n",
            "EPOCH: 24 STEP: 12075 VAL_LOSS: 1.2984243631362915\n",
            "EPOCH: 24 STEP: 12100 VAL_LOSS: 1.3004157543182373\n",
            "EPOCH: 24 STEP: 12125 VAL_LOSS: 1.3022959232330322\n",
            "EPOCH: 24 STEP: 12150 VAL_LOSS: 1.3006446361541748\n",
            "EPOCH: 24 STEP: 12175 VAL_LOSS: 1.3041019439697266\n",
            "EPOCH: 24 STEP: 12200 VAL_LOSS: 1.3016937971115112\n",
            "EPOCH: 24 STEP: 12225 VAL_LOSS: 1.3053029775619507\n",
            "EPOCH: 24 STEP: 12250 VAL_LOSS: 1.311188817024231\n",
            "EPOCH: 25 STEP: 12275 VAL_LOSS: 1.3038604259490967\n",
            "EPOCH: 25 STEP: 12300 VAL_LOSS: 1.3073177337646484\n",
            "EPOCH: 25 STEP: 12325 VAL_LOSS: 1.3060197830200195\n",
            "EPOCH: 25 STEP: 12350 VAL_LOSS: 1.3049404621124268\n",
            "EPOCH: 25 STEP: 12375 VAL_LOSS: 1.3071306943893433\n",
            "EPOCH: 25 STEP: 12400 VAL_LOSS: 1.3091604709625244\n",
            "EPOCH: 25 STEP: 12425 VAL_LOSS: 1.3073618412017822\n",
            "EPOCH: 25 STEP: 12450 VAL_LOSS: 1.31484055519104\n",
            "EPOCH: 25 STEP: 12475 VAL_LOSS: 1.3084816932678223\n",
            "EPOCH: 25 STEP: 12500 VAL_LOSS: 1.3055580854415894\n",
            "EPOCH: 25 STEP: 12525 VAL_LOSS: 1.3070722818374634\n",
            "EPOCH: 25 STEP: 12550 VAL_LOSS: 1.3036022186279297\n",
            "EPOCH: 25 STEP: 12575 VAL_LOSS: 1.3002400398254395\n",
            "EPOCH: 25 STEP: 12600 VAL_LOSS: 1.3017966747283936\n",
            "EPOCH: 25 STEP: 12625 VAL_LOSS: 1.3037670850753784\n",
            "EPOCH: 25 STEP: 12650 VAL_LOSS: 1.3032368421554565\n",
            "EPOCH: 25 STEP: 12675 VAL_LOSS: 1.3073828220367432\n",
            "EPOCH: 25 STEP: 12700 VAL_LOSS: 1.3051824569702148\n",
            "EPOCH: 25 STEP: 12725 VAL_LOSS: 1.3073920011520386\n",
            "EPOCH: 26 STEP: 12750 VAL_LOSS: 1.3071657419204712\n",
            "EPOCH: 26 STEP: 12775 VAL_LOSS: 1.3055403232574463\n",
            "EPOCH: 26 STEP: 12800 VAL_LOSS: 1.3059858083724976\n",
            "EPOCH: 26 STEP: 12825 VAL_LOSS: 1.3034756183624268\n",
            "EPOCH: 26 STEP: 12850 VAL_LOSS: 1.3000075817108154\n",
            "EPOCH: 26 STEP: 12875 VAL_LOSS: 1.3016451597213745\n",
            "EPOCH: 26 STEP: 12900 VAL_LOSS: 1.3088430166244507\n",
            "EPOCH: 26 STEP: 12925 VAL_LOSS: 1.3042244911193848\n",
            "EPOCH: 26 STEP: 12950 VAL_LOSS: 1.3018841743469238\n",
            "EPOCH: 26 STEP: 12975 VAL_LOSS: 1.2979241609573364\n",
            "EPOCH: 26 STEP: 13000 VAL_LOSS: 1.302003026008606\n",
            "EPOCH: 26 STEP: 13025 VAL_LOSS: 1.3046678304672241\n",
            "EPOCH: 26 STEP: 13050 VAL_LOSS: 1.3066352605819702\n",
            "EPOCH: 26 STEP: 13075 VAL_LOSS: 1.298966646194458\n",
            "EPOCH: 26 STEP: 13100 VAL_LOSS: 1.3037594556808472\n",
            "EPOCH: 26 STEP: 13125 VAL_LOSS: 1.301954984664917\n",
            "EPOCH: 26 STEP: 13150 VAL_LOSS: 1.3036704063415527\n",
            "EPOCH: 26 STEP: 13175 VAL_LOSS: 1.3008922338485718\n",
            "EPOCH: 26 STEP: 13200 VAL_LOSS: 1.3023196458816528\n",
            "EPOCH: 26 STEP: 13225 VAL_LOSS: 1.303541660308838\n",
            "EPOCH: 27 STEP: 13250 VAL_LOSS: 1.3039695024490356\n",
            "EPOCH: 27 STEP: 13275 VAL_LOSS: 1.3018369674682617\n",
            "EPOCH: 27 STEP: 13300 VAL_LOSS: 1.3026292324066162\n",
            "EPOCH: 27 STEP: 13325 VAL_LOSS: 1.300941824913025\n",
            "EPOCH: 27 STEP: 13350 VAL_LOSS: 1.3013778924942017\n",
            "EPOCH: 27 STEP: 13375 VAL_LOSS: 1.3008577823638916\n",
            "EPOCH: 27 STEP: 13400 VAL_LOSS: 1.3012635707855225\n",
            "EPOCH: 27 STEP: 13425 VAL_LOSS: 1.301466464996338\n",
            "EPOCH: 27 STEP: 13450 VAL_LOSS: 1.302985429763794\n",
            "EPOCH: 27 STEP: 13475 VAL_LOSS: 1.2978546619415283\n",
            "EPOCH: 27 STEP: 13500 VAL_LOSS: 1.2955790758132935\n",
            "EPOCH: 27 STEP: 13525 VAL_LOSS: 1.2946990728378296\n",
            "EPOCH: 27 STEP: 13550 VAL_LOSS: 1.2921855449676514\n",
            "EPOCH: 27 STEP: 13575 VAL_LOSS: 1.2999486923217773\n",
            "EPOCH: 27 STEP: 13600 VAL_LOSS: 1.296588659286499\n",
            "EPOCH: 27 STEP: 13625 VAL_LOSS: 1.3028002977371216\n",
            "EPOCH: 27 STEP: 13650 VAL_LOSS: 1.3056894540786743\n",
            "EPOCH: 27 STEP: 13675 VAL_LOSS: 1.301743984222412\n",
            "EPOCH: 27 STEP: 13700 VAL_LOSS: 1.3032125234603882\n",
            "EPOCH: 28 STEP: 13725 VAL_LOSS: 1.3075069189071655\n",
            "EPOCH: 28 STEP: 13750 VAL_LOSS: 1.3070534467697144\n",
            "EPOCH: 28 STEP: 13775 VAL_LOSS: 1.30536949634552\n",
            "EPOCH: 28 STEP: 13800 VAL_LOSS: 1.3059027194976807\n",
            "EPOCH: 28 STEP: 13825 VAL_LOSS: 1.3099523782730103\n",
            "EPOCH: 28 STEP: 13850 VAL_LOSS: 1.3058836460113525\n",
            "EPOCH: 28 STEP: 13875 VAL_LOSS: 1.3075155019760132\n",
            "EPOCH: 28 STEP: 13900 VAL_LOSS: 1.3028966188430786\n",
            "EPOCH: 28 STEP: 13925 VAL_LOSS: 1.308559536933899\n",
            "EPOCH: 28 STEP: 13950 VAL_LOSS: 1.302249550819397\n",
            "EPOCH: 28 STEP: 13975 VAL_LOSS: 1.2997099161148071\n",
            "EPOCH: 28 STEP: 14000 VAL_LOSS: 1.2996243238449097\n",
            "EPOCH: 28 STEP: 14025 VAL_LOSS: 1.3001248836517334\n",
            "EPOCH: 28 STEP: 14050 VAL_LOSS: 1.3019530773162842\n",
            "EPOCH: 28 STEP: 14075 VAL_LOSS: 1.3054019212722778\n",
            "EPOCH: 28 STEP: 14100 VAL_LOSS: 1.3038028478622437\n",
            "EPOCH: 28 STEP: 14125 VAL_LOSS: 1.3028477430343628\n",
            "EPOCH: 28 STEP: 14150 VAL_LOSS: 1.3101848363876343\n",
            "EPOCH: 28 STEP: 14175 VAL_LOSS: 1.3070697784423828\n",
            "EPOCH: 28 STEP: 14200 VAL_LOSS: 1.3102480173110962\n",
            "EPOCH: 29 STEP: 14225 VAL_LOSS: 1.3001182079315186\n",
            "EPOCH: 29 STEP: 14250 VAL_LOSS: 1.3010821342468262\n",
            "EPOCH: 29 STEP: 14275 VAL_LOSS: 1.3048977851867676\n",
            "EPOCH: 29 STEP: 14300 VAL_LOSS: 1.3030507564544678\n",
            "EPOCH: 29 STEP: 14325 VAL_LOSS: 1.3064926862716675\n",
            "EPOCH: 29 STEP: 14350 VAL_LOSS: 1.311438798904419\n",
            "EPOCH: 29 STEP: 14375 VAL_LOSS: 1.317098617553711\n",
            "EPOCH: 29 STEP: 14400 VAL_LOSS: 1.3084821701049805\n",
            "EPOCH: 29 STEP: 14425 VAL_LOSS: 1.3074051141738892\n",
            "EPOCH: 29 STEP: 14450 VAL_LOSS: 1.3044898509979248\n",
            "EPOCH: 29 STEP: 14475 VAL_LOSS: 1.3032523393630981\n",
            "EPOCH: 29 STEP: 14500 VAL_LOSS: 1.3025277853012085\n",
            "EPOCH: 29 STEP: 14525 VAL_LOSS: 1.3040071725845337\n",
            "EPOCH: 29 STEP: 14550 VAL_LOSS: 1.299766182899475\n",
            "EPOCH: 29 STEP: 14575 VAL_LOSS: 1.3057500123977661\n",
            "EPOCH: 29 STEP: 14600 VAL_LOSS: 1.3075661659240723\n",
            "EPOCH: 29 STEP: 14625 VAL_LOSS: 1.3012334108352661\n",
            "EPOCH: 29 STEP: 14650 VAL_LOSS: 1.3031316995620728\n",
            "EPOCH: 29 STEP: 14675 VAL_LOSS: 1.3054180145263672\n",
            "EPOCH: 29 STEP: 14700 VAL_LOSS: 1.3034299612045288\n",
            "EPOCH: 30 STEP: 14725 VAL_LOSS: 1.303139090538025\n",
            "EPOCH: 30 STEP: 14750 VAL_LOSS: 1.303990125656128\n",
            "EPOCH: 30 STEP: 14775 VAL_LOSS: 1.3072212934494019\n",
            "EPOCH: 30 STEP: 14800 VAL_LOSS: 1.3066017627716064\n",
            "EPOCH: 30 STEP: 14825 VAL_LOSS: 1.3036454916000366\n",
            "EPOCH: 30 STEP: 14850 VAL_LOSS: 1.3077870607376099\n",
            "EPOCH: 30 STEP: 14875 VAL_LOSS: 1.3074134588241577\n",
            "EPOCH: 30 STEP: 14900 VAL_LOSS: 1.3092705011367798\n",
            "EPOCH: 30 STEP: 14925 VAL_LOSS: 1.3026772737503052\n",
            "EPOCH: 30 STEP: 14950 VAL_LOSS: 1.304434061050415\n",
            "EPOCH: 30 STEP: 14975 VAL_LOSS: 1.3043828010559082\n",
            "EPOCH: 30 STEP: 15000 VAL_LOSS: 1.309322476387024\n",
            "EPOCH: 30 STEP: 15025 VAL_LOSS: 1.2965446710586548\n",
            "EPOCH: 30 STEP: 15050 VAL_LOSS: 1.3013662099838257\n",
            "EPOCH: 30 STEP: 15075 VAL_LOSS: 1.298060417175293\n",
            "EPOCH: 30 STEP: 15100 VAL_LOSS: 1.3040530681610107\n",
            "EPOCH: 30 STEP: 15125 VAL_LOSS: 1.3060221672058105\n",
            "EPOCH: 30 STEP: 15150 VAL_LOSS: 1.3060789108276367\n",
            "EPOCH: 30 STEP: 15175 VAL_LOSS: 1.3111040592193604\n",
            "EPOCH: 31 STEP: 15200 VAL_LOSS: 1.3066179752349854\n",
            "EPOCH: 31 STEP: 15225 VAL_LOSS: 1.3022621870040894\n",
            "EPOCH: 31 STEP: 15250 VAL_LOSS: 1.307520866394043\n",
            "EPOCH: 31 STEP: 15275 VAL_LOSS: 1.3116209506988525\n",
            "EPOCH: 31 STEP: 15300 VAL_LOSS: 1.3072501420974731\n",
            "EPOCH: 31 STEP: 15325 VAL_LOSS: 1.3021037578582764\n",
            "EPOCH: 31 STEP: 15350 VAL_LOSS: 1.307888388633728\n",
            "EPOCH: 31 STEP: 15375 VAL_LOSS: 1.3022527694702148\n",
            "EPOCH: 31 STEP: 15400 VAL_LOSS: 1.308119297027588\n",
            "EPOCH: 31 STEP: 15425 VAL_LOSS: 1.3041584491729736\n",
            "EPOCH: 31 STEP: 15450 VAL_LOSS: 1.3012555837631226\n",
            "EPOCH: 31 STEP: 15475 VAL_LOSS: 1.30307936668396\n",
            "EPOCH: 31 STEP: 15500 VAL_LOSS: 1.2946349382400513\n",
            "EPOCH: 31 STEP: 15525 VAL_LOSS: 1.297086477279663\n",
            "EPOCH: 31 STEP: 15550 VAL_LOSS: 1.303836703300476\n",
            "EPOCH: 31 STEP: 15575 VAL_LOSS: 1.3044933080673218\n",
            "EPOCH: 31 STEP: 15600 VAL_LOSS: 1.3053754568099976\n",
            "EPOCH: 31 STEP: 15625 VAL_LOSS: 1.306350827217102\n",
            "EPOCH: 31 STEP: 15650 VAL_LOSS: 1.3059862852096558\n",
            "EPOCH: 31 STEP: 15675 VAL_LOSS: 1.305236577987671\n",
            "EPOCH: 32 STEP: 15700 VAL_LOSS: 1.308281421661377\n",
            "EPOCH: 32 STEP: 15725 VAL_LOSS: 1.309325098991394\n",
            "EPOCH: 32 STEP: 15750 VAL_LOSS: 1.310927391052246\n",
            "EPOCH: 32 STEP: 15775 VAL_LOSS: 1.3085545301437378\n",
            "EPOCH: 32 STEP: 15800 VAL_LOSS: 1.3124324083328247\n",
            "EPOCH: 32 STEP: 15825 VAL_LOSS: 1.3039212226867676\n",
            "EPOCH: 32 STEP: 15850 VAL_LOSS: 1.3068835735321045\n",
            "EPOCH: 32 STEP: 15875 VAL_LOSS: 1.3074859380722046\n",
            "EPOCH: 32 STEP: 15900 VAL_LOSS: 1.312621831893921\n",
            "EPOCH: 32 STEP: 15925 VAL_LOSS: 1.305860996246338\n",
            "EPOCH: 32 STEP: 15950 VAL_LOSS: 1.3099011182785034\n",
            "EPOCH: 32 STEP: 15975 VAL_LOSS: 1.3093410730361938\n",
            "EPOCH: 32 STEP: 16000 VAL_LOSS: 1.2988160848617554\n",
            "EPOCH: 32 STEP: 16025 VAL_LOSS: 1.3030402660369873\n",
            "EPOCH: 32 STEP: 16050 VAL_LOSS: 1.3032641410827637\n",
            "EPOCH: 32 STEP: 16075 VAL_LOSS: 1.3073209524154663\n",
            "EPOCH: 32 STEP: 16100 VAL_LOSS: 1.3094675540924072\n",
            "EPOCH: 32 STEP: 16125 VAL_LOSS: 1.3079354763031006\n",
            "EPOCH: 32 STEP: 16150 VAL_LOSS: 1.3099194765090942\n",
            "EPOCH: 33 STEP: 16175 VAL_LOSS: 1.315169095993042\n",
            "EPOCH: 33 STEP: 16200 VAL_LOSS: 1.3108986616134644\n",
            "EPOCH: 33 STEP: 16225 VAL_LOSS: 1.307307481765747\n",
            "EPOCH: 33 STEP: 16250 VAL_LOSS: 1.3061604499816895\n",
            "EPOCH: 33 STEP: 16275 VAL_LOSS: 1.311698079109192\n",
            "EPOCH: 33 STEP: 16300 VAL_LOSS: 1.3064559698104858\n",
            "EPOCH: 33 STEP: 16325 VAL_LOSS: 1.31282639503479\n",
            "EPOCH: 33 STEP: 16350 VAL_LOSS: 1.3147255182266235\n",
            "EPOCH: 33 STEP: 16375 VAL_LOSS: 1.3126331567764282\n",
            "EPOCH: 33 STEP: 16400 VAL_LOSS: 1.3104082345962524\n",
            "EPOCH: 33 STEP: 16425 VAL_LOSS: 1.3116763830184937\n",
            "EPOCH: 33 STEP: 16450 VAL_LOSS: 1.3173234462738037\n",
            "EPOCH: 33 STEP: 16475 VAL_LOSS: 1.3129637241363525\n",
            "EPOCH: 33 STEP: 16500 VAL_LOSS: 1.3095576763153076\n",
            "EPOCH: 33 STEP: 16525 VAL_LOSS: 1.3087397813796997\n",
            "EPOCH: 33 STEP: 16550 VAL_LOSS: 1.3117114305496216\n",
            "EPOCH: 33 STEP: 16575 VAL_LOSS: 1.3152390718460083\n",
            "EPOCH: 33 STEP: 16600 VAL_LOSS: 1.3156108856201172\n",
            "EPOCH: 33 STEP: 16625 VAL_LOSS: 1.3143476247787476\n",
            "EPOCH: 33 STEP: 16650 VAL_LOSS: 1.313189148902893\n",
            "EPOCH: 34 STEP: 16675 VAL_LOSS: 1.3060258626937866\n",
            "EPOCH: 34 STEP: 16700 VAL_LOSS: 1.3065054416656494\n",
            "EPOCH: 34 STEP: 16725 VAL_LOSS: 1.310441255569458\n",
            "EPOCH: 34 STEP: 16750 VAL_LOSS: 1.3111063241958618\n",
            "EPOCH: 34 STEP: 16775 VAL_LOSS: 1.31560480594635\n",
            "EPOCH: 34 STEP: 16800 VAL_LOSS: 1.3144452571868896\n",
            "EPOCH: 34 STEP: 16825 VAL_LOSS: 1.3212082386016846\n",
            "EPOCH: 34 STEP: 16850 VAL_LOSS: 1.3138236999511719\n",
            "EPOCH: 34 STEP: 16875 VAL_LOSS: 1.3104013204574585\n",
            "EPOCH: 34 STEP: 16900 VAL_LOSS: 1.313132405281067\n",
            "EPOCH: 34 STEP: 16925 VAL_LOSS: 1.3072842359542847\n",
            "EPOCH: 34 STEP: 16950 VAL_LOSS: 1.314084529876709\n",
            "EPOCH: 34 STEP: 16975 VAL_LOSS: 1.313801884651184\n",
            "EPOCH: 34 STEP: 17000 VAL_LOSS: 1.308068871498108\n",
            "EPOCH: 34 STEP: 17025 VAL_LOSS: 1.3106544017791748\n",
            "EPOCH: 34 STEP: 17050 VAL_LOSS: 1.3108015060424805\n",
            "EPOCH: 34 STEP: 17075 VAL_LOSS: 1.3089752197265625\n",
            "EPOCH: 34 STEP: 17100 VAL_LOSS: 1.3047511577606201\n",
            "EPOCH: 34 STEP: 17125 VAL_LOSS: 1.310464859008789\n",
            "EPOCH: 34 STEP: 17150 VAL_LOSS: 1.3108611106872559\n",
            "EPOCH: 35 STEP: 17175 VAL_LOSS: 1.3083010911941528\n",
            "EPOCH: 35 STEP: 17200 VAL_LOSS: 1.3019168376922607\n",
            "EPOCH: 35 STEP: 17225 VAL_LOSS: 1.3130557537078857\n",
            "EPOCH: 35 STEP: 17250 VAL_LOSS: 1.3097784519195557\n",
            "EPOCH: 35 STEP: 17275 VAL_LOSS: 1.306504249572754\n",
            "EPOCH: 35 STEP: 17300 VAL_LOSS: 1.3102471828460693\n",
            "EPOCH: 35 STEP: 17325 VAL_LOSS: 1.3106697797775269\n",
            "EPOCH: 35 STEP: 17350 VAL_LOSS: 1.3150502443313599\n",
            "EPOCH: 35 STEP: 17375 VAL_LOSS: 1.3175644874572754\n",
            "EPOCH: 35 STEP: 17400 VAL_LOSS: 1.3092924356460571\n",
            "EPOCH: 35 STEP: 17425 VAL_LOSS: 1.3148833513259888\n",
            "EPOCH: 35 STEP: 17450 VAL_LOSS: 1.3103556632995605\n",
            "EPOCH: 35 STEP: 17475 VAL_LOSS: 1.3065961599349976\n",
            "EPOCH: 35 STEP: 17500 VAL_LOSS: 1.314246654510498\n",
            "EPOCH: 35 STEP: 17525 VAL_LOSS: 1.3076670169830322\n",
            "EPOCH: 35 STEP: 17550 VAL_LOSS: 1.3074613809585571\n",
            "EPOCH: 35 STEP: 17575 VAL_LOSS: 1.3138823509216309\n",
            "EPOCH: 35 STEP: 17600 VAL_LOSS: 1.3091367483139038\n",
            "EPOCH: 35 STEP: 17625 VAL_LOSS: 1.3105964660644531\n",
            "EPOCH: 36 STEP: 17650 VAL_LOSS: 1.3104662895202637\n",
            "EPOCH: 36 STEP: 17675 VAL_LOSS: 1.311828851699829\n",
            "EPOCH: 36 STEP: 17700 VAL_LOSS: 1.3155453205108643\n",
            "EPOCH: 36 STEP: 17725 VAL_LOSS: 1.3090780973434448\n",
            "EPOCH: 36 STEP: 17750 VAL_LOSS: 1.3122538328170776\n",
            "EPOCH: 36 STEP: 17775 VAL_LOSS: 1.3117892742156982\n",
            "EPOCH: 36 STEP: 17800 VAL_LOSS: 1.3131422996520996\n",
            "EPOCH: 36 STEP: 17825 VAL_LOSS: 1.3148596286773682\n",
            "EPOCH: 36 STEP: 17850 VAL_LOSS: 1.3115301132202148\n",
            "EPOCH: 36 STEP: 17875 VAL_LOSS: 1.3111076354980469\n",
            "EPOCH: 36 STEP: 17900 VAL_LOSS: 1.3098795413970947\n",
            "EPOCH: 36 STEP: 17925 VAL_LOSS: 1.3090122938156128\n",
            "EPOCH: 36 STEP: 17950 VAL_LOSS: 1.3131396770477295\n",
            "EPOCH: 36 STEP: 17975 VAL_LOSS: 1.307998538017273\n",
            "EPOCH: 36 STEP: 18000 VAL_LOSS: 1.3116333484649658\n",
            "EPOCH: 36 STEP: 18025 VAL_LOSS: 1.313939094543457\n",
            "EPOCH: 36 STEP: 18050 VAL_LOSS: 1.316091775894165\n",
            "EPOCH: 36 STEP: 18075 VAL_LOSS: 1.3147196769714355\n",
            "EPOCH: 36 STEP: 18100 VAL_LOSS: 1.313830018043518\n",
            "EPOCH: 36 STEP: 18125 VAL_LOSS: 1.3102644681930542\n",
            "EPOCH: 37 STEP: 18150 VAL_LOSS: 1.3132208585739136\n",
            "EPOCH: 37 STEP: 18175 VAL_LOSS: 1.3107974529266357\n",
            "EPOCH: 37 STEP: 18200 VAL_LOSS: 1.3115386962890625\n",
            "EPOCH: 37 STEP: 18225 VAL_LOSS: 1.312003493309021\n",
            "EPOCH: 37 STEP: 18250 VAL_LOSS: 1.3170843124389648\n",
            "EPOCH: 37 STEP: 18275 VAL_LOSS: 1.3177789449691772\n",
            "EPOCH: 37 STEP: 18300 VAL_LOSS: 1.3117128610610962\n",
            "EPOCH: 37 STEP: 18325 VAL_LOSS: 1.310224175453186\n",
            "EPOCH: 37 STEP: 18350 VAL_LOSS: 1.3132742643356323\n",
            "EPOCH: 37 STEP: 18375 VAL_LOSS: 1.3098363876342773\n",
            "EPOCH: 37 STEP: 18400 VAL_LOSS: 1.3110889196395874\n",
            "EPOCH: 37 STEP: 18425 VAL_LOSS: 1.3130440711975098\n",
            "EPOCH: 37 STEP: 18450 VAL_LOSS: 1.3063960075378418\n",
            "EPOCH: 37 STEP: 18475 VAL_LOSS: 1.307969093322754\n",
            "EPOCH: 37 STEP: 18500 VAL_LOSS: 1.3106536865234375\n",
            "EPOCH: 37 STEP: 18525 VAL_LOSS: 1.3123823404312134\n",
            "EPOCH: 37 STEP: 18550 VAL_LOSS: 1.3160948753356934\n",
            "EPOCH: 37 STEP: 18575 VAL_LOSS: 1.307681679725647\n",
            "EPOCH: 37 STEP: 18600 VAL_LOSS: 1.3108763694763184\n",
            "EPOCH: 38 STEP: 18625 VAL_LOSS: 1.3108209371566772\n",
            "EPOCH: 38 STEP: 18650 VAL_LOSS: 1.307082176208496\n",
            "EPOCH: 38 STEP: 18675 VAL_LOSS: 1.3088970184326172\n",
            "EPOCH: 38 STEP: 18700 VAL_LOSS: 1.307369589805603\n",
            "EPOCH: 38 STEP: 18725 VAL_LOSS: 1.3087111711502075\n",
            "EPOCH: 38 STEP: 18750 VAL_LOSS: 1.3080145120620728\n",
            "EPOCH: 38 STEP: 18775 VAL_LOSS: 1.3042621612548828\n",
            "EPOCH: 38 STEP: 18800 VAL_LOSS: 1.3076353073120117\n",
            "EPOCH: 38 STEP: 18825 VAL_LOSS: 1.3062372207641602\n",
            "EPOCH: 38 STEP: 18850 VAL_LOSS: 1.3091782331466675\n",
            "EPOCH: 38 STEP: 18875 VAL_LOSS: 1.3075376749038696\n",
            "EPOCH: 38 STEP: 18900 VAL_LOSS: 1.3093701601028442\n",
            "EPOCH: 38 STEP: 18925 VAL_LOSS: 1.3114107847213745\n",
            "EPOCH: 38 STEP: 18950 VAL_LOSS: 1.3109937906265259\n",
            "EPOCH: 38 STEP: 18975 VAL_LOSS: 1.3078720569610596\n",
            "EPOCH: 38 STEP: 19000 VAL_LOSS: 1.3070318698883057\n",
            "EPOCH: 38 STEP: 19025 VAL_LOSS: 1.3073819875717163\n",
            "EPOCH: 38 STEP: 19050 VAL_LOSS: 1.3127262592315674\n",
            "EPOCH: 38 STEP: 19075 VAL_LOSS: 1.3080610036849976\n",
            "EPOCH: 38 STEP: 19100 VAL_LOSS: 1.3077696561813354\n",
            "EPOCH: 39 STEP: 19125 VAL_LOSS: 1.3118520975112915\n",
            "EPOCH: 39 STEP: 19150 VAL_LOSS: 1.3112831115722656\n",
            "EPOCH: 39 STEP: 19175 VAL_LOSS: 1.309255599975586\n",
            "EPOCH: 39 STEP: 19200 VAL_LOSS: 1.3067437410354614\n",
            "EPOCH: 39 STEP: 19225 VAL_LOSS: 1.3107210397720337\n",
            "EPOCH: 39 STEP: 19250 VAL_LOSS: 1.3100714683532715\n",
            "EPOCH: 39 STEP: 19275 VAL_LOSS: 1.3110898733139038\n",
            "EPOCH: 39 STEP: 19300 VAL_LOSS: 1.3026816844940186\n",
            "EPOCH: 39 STEP: 19325 VAL_LOSS: 1.3026930093765259\n",
            "EPOCH: 39 STEP: 19350 VAL_LOSS: 1.3132877349853516\n",
            "EPOCH: 39 STEP: 19375 VAL_LOSS: 1.305943250656128\n",
            "EPOCH: 39 STEP: 19400 VAL_LOSS: 1.3069711923599243\n",
            "EPOCH: 39 STEP: 19425 VAL_LOSS: 1.3094894886016846\n",
            "EPOCH: 39 STEP: 19450 VAL_LOSS: 1.3055957555770874\n",
            "EPOCH: 39 STEP: 19475 VAL_LOSS: 1.3045761585235596\n",
            "EPOCH: 39 STEP: 19500 VAL_LOSS: 1.3096184730529785\n",
            "EPOCH: 39 STEP: 19525 VAL_LOSS: 1.3277508020401\n",
            "EPOCH: 39 STEP: 19550 VAL_LOSS: 1.3071980476379395\n",
            "EPOCH: 39 STEP: 19575 VAL_LOSS: 1.2992767095565796\n",
            "EPOCH: 39 STEP: 19600 VAL_LOSS: 1.2992581129074097\n",
            "EPOCH: 40 STEP: 19625 VAL_LOSS: 1.2986640930175781\n",
            "EPOCH: 40 STEP: 19650 VAL_LOSS: 1.3032567501068115\n",
            "EPOCH: 40 STEP: 19675 VAL_LOSS: 1.306885004043579\n",
            "EPOCH: 40 STEP: 19700 VAL_LOSS: 1.3080406188964844\n",
            "EPOCH: 40 STEP: 19725 VAL_LOSS: 1.3055516481399536\n",
            "EPOCH: 40 STEP: 19750 VAL_LOSS: 1.3046547174453735\n",
            "EPOCH: 40 STEP: 19775 VAL_LOSS: 1.3029710054397583\n",
            "EPOCH: 40 STEP: 19800 VAL_LOSS: 1.305728793144226\n",
            "EPOCH: 40 STEP: 19825 VAL_LOSS: 1.3063337802886963\n",
            "EPOCH: 40 STEP: 19850 VAL_LOSS: 1.3027944564819336\n",
            "EPOCH: 40 STEP: 19875 VAL_LOSS: 1.3092217445373535\n",
            "EPOCH: 40 STEP: 19900 VAL_LOSS: 1.3123522996902466\n",
            "EPOCH: 40 STEP: 19925 VAL_LOSS: 1.3050183057785034\n",
            "EPOCH: 40 STEP: 19950 VAL_LOSS: 1.3109341859817505\n",
            "EPOCH: 40 STEP: 19975 VAL_LOSS: 1.312312364578247\n",
            "EPOCH: 40 STEP: 20000 VAL_LOSS: 1.3099626302719116\n",
            "EPOCH: 40 STEP: 20025 VAL_LOSS: 1.3081282377243042\n",
            "EPOCH: 40 STEP: 20050 VAL_LOSS: 1.3083690404891968\n",
            "EPOCH: 40 STEP: 20075 VAL_LOSS: 1.3069233894348145\n",
            "EPOCH: 41 STEP: 20100 VAL_LOSS: 1.3072820901870728\n",
            "EPOCH: 41 STEP: 20125 VAL_LOSS: 1.3044990301132202\n",
            "EPOCH: 41 STEP: 20150 VAL_LOSS: 1.311815619468689\n",
            "EPOCH: 41 STEP: 20175 VAL_LOSS: 1.310992956161499\n",
            "EPOCH: 41 STEP: 20200 VAL_LOSS: 1.3115363121032715\n",
            "EPOCH: 41 STEP: 20225 VAL_LOSS: 1.309676170349121\n",
            "EPOCH: 41 STEP: 20250 VAL_LOSS: 1.3111358880996704\n",
            "EPOCH: 41 STEP: 20275 VAL_LOSS: 1.3113573789596558\n",
            "EPOCH: 41 STEP: 20300 VAL_LOSS: 1.3060489892959595\n",
            "EPOCH: 41 STEP: 20325 VAL_LOSS: 1.311171054840088\n",
            "EPOCH: 41 STEP: 20350 VAL_LOSS: 1.307090401649475\n",
            "EPOCH: 41 STEP: 20375 VAL_LOSS: 1.3119279146194458\n",
            "EPOCH: 41 STEP: 20400 VAL_LOSS: 1.3057211637496948\n",
            "EPOCH: 41 STEP: 20425 VAL_LOSS: 1.3037426471710205\n",
            "EPOCH: 41 STEP: 20450 VAL_LOSS: 1.3070565462112427\n",
            "EPOCH: 41 STEP: 20475 VAL_LOSS: 1.3099480867385864\n",
            "EPOCH: 41 STEP: 20500 VAL_LOSS: 1.3075088262557983\n",
            "EPOCH: 41 STEP: 20525 VAL_LOSS: 1.3046185970306396\n",
            "EPOCH: 41 STEP: 20550 VAL_LOSS: 1.305942177772522\n",
            "EPOCH: 41 STEP: 20575 VAL_LOSS: 1.304466962814331\n",
            "EPOCH: 42 STEP: 20600 VAL_LOSS: 1.3067704439163208\n",
            "EPOCH: 42 STEP: 20625 VAL_LOSS: 1.305533528327942\n",
            "EPOCH: 42 STEP: 20650 VAL_LOSS: 1.3084608316421509\n",
            "EPOCH: 42 STEP: 20675 VAL_LOSS: 1.3055217266082764\n",
            "EPOCH: 42 STEP: 20700 VAL_LOSS: 1.309566855430603\n",
            "EPOCH: 42 STEP: 20725 VAL_LOSS: 1.3090733289718628\n",
            "EPOCH: 42 STEP: 20750 VAL_LOSS: 1.3129466772079468\n",
            "EPOCH: 42 STEP: 20775 VAL_LOSS: 1.308821678161621\n",
            "EPOCH: 42 STEP: 20800 VAL_LOSS: 1.3173519372940063\n",
            "EPOCH: 42 STEP: 20825 VAL_LOSS: 1.312605857849121\n",
            "EPOCH: 42 STEP: 20850 VAL_LOSS: 1.3135907649993896\n",
            "EPOCH: 42 STEP: 20875 VAL_LOSS: 1.3059958219528198\n",
            "EPOCH: 42 STEP: 20900 VAL_LOSS: 1.3082609176635742\n",
            "EPOCH: 42 STEP: 20925 VAL_LOSS: 1.3066669702529907\n",
            "EPOCH: 42 STEP: 20950 VAL_LOSS: 1.3088488578796387\n",
            "EPOCH: 42 STEP: 20975 VAL_LOSS: 1.311280608177185\n",
            "EPOCH: 42 STEP: 21000 VAL_LOSS: 1.31034255027771\n",
            "EPOCH: 42 STEP: 21025 VAL_LOSS: 1.3074935674667358\n",
            "EPOCH: 42 STEP: 21050 VAL_LOSS: 1.3097800016403198\n",
            "EPOCH: 43 STEP: 21075 VAL_LOSS: 1.3126537799835205\n",
            "EPOCH: 43 STEP: 21100 VAL_LOSS: 1.3115280866622925\n",
            "EPOCH: 43 STEP: 21125 VAL_LOSS: 1.312165379524231\n",
            "EPOCH: 43 STEP: 21150 VAL_LOSS: 1.3106988668441772\n",
            "EPOCH: 43 STEP: 21175 VAL_LOSS: 1.3162031173706055\n",
            "EPOCH: 43 STEP: 21200 VAL_LOSS: 1.3133931159973145\n",
            "EPOCH: 43 STEP: 21225 VAL_LOSS: 1.3129340410232544\n",
            "EPOCH: 43 STEP: 21250 VAL_LOSS: 1.3098673820495605\n",
            "EPOCH: 43 STEP: 21275 VAL_LOSS: 1.319564700126648\n",
            "EPOCH: 43 STEP: 21300 VAL_LOSS: 1.310949683189392\n",
            "EPOCH: 43 STEP: 21325 VAL_LOSS: 1.3115227222442627\n",
            "EPOCH: 43 STEP: 21350 VAL_LOSS: 1.316543698310852\n",
            "EPOCH: 43 STEP: 21375 VAL_LOSS: 1.3134907484054565\n",
            "EPOCH: 43 STEP: 21400 VAL_LOSS: 1.3133524656295776\n",
            "EPOCH: 43 STEP: 21425 VAL_LOSS: 1.3144155740737915\n",
            "EPOCH: 43 STEP: 21450 VAL_LOSS: 1.3132108449935913\n",
            "EPOCH: 43 STEP: 21475 VAL_LOSS: 1.3086475133895874\n",
            "EPOCH: 43 STEP: 21500 VAL_LOSS: 1.3100006580352783\n",
            "EPOCH: 43 STEP: 21525 VAL_LOSS: 1.3050259351730347\n",
            "EPOCH: 43 STEP: 21550 VAL_LOSS: 1.3068971633911133\n",
            "EPOCH: 44 STEP: 21575 VAL_LOSS: 1.309874176979065\n",
            "EPOCH: 44 STEP: 21600 VAL_LOSS: 1.3095006942749023\n",
            "EPOCH: 44 STEP: 21625 VAL_LOSS: 1.3100383281707764\n",
            "EPOCH: 44 STEP: 21650 VAL_LOSS: 1.3091994524002075\n",
            "EPOCH: 44 STEP: 21675 VAL_LOSS: 1.3117002248764038\n",
            "EPOCH: 44 STEP: 21700 VAL_LOSS: 1.3127000331878662\n",
            "EPOCH: 44 STEP: 21725 VAL_LOSS: 1.3164902925491333\n",
            "EPOCH: 44 STEP: 21750 VAL_LOSS: 1.3122600317001343\n",
            "EPOCH: 44 STEP: 21775 VAL_LOSS: 1.3111631870269775\n",
            "EPOCH: 44 STEP: 21800 VAL_LOSS: 1.3138550519943237\n",
            "EPOCH: 44 STEP: 21825 VAL_LOSS: 1.309032678604126\n",
            "EPOCH: 44 STEP: 21850 VAL_LOSS: 1.3137476444244385\n",
            "EPOCH: 44 STEP: 21875 VAL_LOSS: 1.3131015300750732\n",
            "EPOCH: 44 STEP: 21900 VAL_LOSS: 1.307815432548523\n",
            "EPOCH: 44 STEP: 21925 VAL_LOSS: 1.311482310295105\n",
            "EPOCH: 44 STEP: 21950 VAL_LOSS: 1.314953088760376\n",
            "EPOCH: 44 STEP: 21975 VAL_LOSS: 1.3136900663375854\n",
            "EPOCH: 44 STEP: 22000 VAL_LOSS: 1.307166337966919\n",
            "EPOCH: 44 STEP: 22025 VAL_LOSS: 1.3145333528518677\n",
            "EPOCH: 44 STEP: 22050 VAL_LOSS: 1.3062785863876343\n",
            "EPOCH: 45 STEP: 22075 VAL_LOSS: 1.3095425367355347\n",
            "EPOCH: 45 STEP: 22100 VAL_LOSS: 1.3101633787155151\n",
            "EPOCH: 45 STEP: 22125 VAL_LOSS: 1.3098121881484985\n",
            "EPOCH: 45 STEP: 22150 VAL_LOSS: 1.3100377321243286\n",
            "EPOCH: 45 STEP: 22175 VAL_LOSS: 1.3123502731323242\n",
            "EPOCH: 45 STEP: 22200 VAL_LOSS: 1.3128681182861328\n",
            "EPOCH: 45 STEP: 22225 VAL_LOSS: 1.312551498413086\n",
            "EPOCH: 45 STEP: 22250 VAL_LOSS: 1.3152403831481934\n",
            "EPOCH: 45 STEP: 22275 VAL_LOSS: 1.3157525062561035\n",
            "EPOCH: 45 STEP: 22300 VAL_LOSS: 1.3116953372955322\n",
            "EPOCH: 45 STEP: 22325 VAL_LOSS: 1.31607186794281\n",
            "EPOCH: 45 STEP: 22350 VAL_LOSS: 1.311737060546875\n",
            "EPOCH: 45 STEP: 22375 VAL_LOSS: 1.303193211555481\n",
            "EPOCH: 45 STEP: 22400 VAL_LOSS: 1.3074804544448853\n",
            "EPOCH: 45 STEP: 22425 VAL_LOSS: 1.3090707063674927\n",
            "EPOCH: 45 STEP: 22450 VAL_LOSS: 1.311880350112915\n",
            "EPOCH: 45 STEP: 22475 VAL_LOSS: 1.308989405632019\n",
            "EPOCH: 45 STEP: 22500 VAL_LOSS: 1.3048955202102661\n",
            "EPOCH: 45 STEP: 22525 VAL_LOSS: 1.311735987663269\n",
            "EPOCH: 46 STEP: 22550 VAL_LOSS: 1.3130160570144653\n",
            "EPOCH: 46 STEP: 22575 VAL_LOSS: 1.3113306760787964\n",
            "EPOCH: 46 STEP: 22600 VAL_LOSS: 1.3132898807525635\n",
            "EPOCH: 46 STEP: 22625 VAL_LOSS: 1.3089467287063599\n",
            "EPOCH: 46 STEP: 22650 VAL_LOSS: 1.30972421169281\n",
            "EPOCH: 46 STEP: 22675 VAL_LOSS: 1.3078210353851318\n",
            "EPOCH: 46 STEP: 22700 VAL_LOSS: 1.313887119293213\n",
            "EPOCH: 46 STEP: 22725 VAL_LOSS: 1.3083504438400269\n",
            "EPOCH: 46 STEP: 22750 VAL_LOSS: 1.3110084533691406\n",
            "EPOCH: 46 STEP: 22775 VAL_LOSS: 1.308882236480713\n",
            "EPOCH: 46 STEP: 22800 VAL_LOSS: 1.3079931735992432\n",
            "EPOCH: 46 STEP: 22825 VAL_LOSS: 1.3162012100219727\n",
            "EPOCH: 46 STEP: 22850 VAL_LOSS: 1.3106764554977417\n",
            "EPOCH: 46 STEP: 22875 VAL_LOSS: 1.30955970287323\n",
            "EPOCH: 46 STEP: 22900 VAL_LOSS: 1.3120300769805908\n",
            "EPOCH: 46 STEP: 22925 VAL_LOSS: 1.313696026802063\n",
            "EPOCH: 46 STEP: 22950 VAL_LOSS: 1.3125920295715332\n",
            "EPOCH: 46 STEP: 22975 VAL_LOSS: 1.3110681772232056\n",
            "EPOCH: 46 STEP: 23000 VAL_LOSS: 1.308987021446228\n",
            "EPOCH: 46 STEP: 23025 VAL_LOSS: 1.3111845254898071\n",
            "EPOCH: 47 STEP: 23050 VAL_LOSS: 1.3067466020584106\n",
            "EPOCH: 47 STEP: 23075 VAL_LOSS: 1.314232587814331\n",
            "EPOCH: 47 STEP: 23100 VAL_LOSS: 1.3098450899124146\n",
            "EPOCH: 47 STEP: 23125 VAL_LOSS: 1.306489109992981\n",
            "EPOCH: 47 STEP: 23150 VAL_LOSS: 1.3148692846298218\n",
            "EPOCH: 47 STEP: 23175 VAL_LOSS: 1.309615969657898\n",
            "EPOCH: 47 STEP: 23200 VAL_LOSS: 1.3123142719268799\n",
            "EPOCH: 47 STEP: 23225 VAL_LOSS: 1.3092422485351562\n",
            "EPOCH: 47 STEP: 23250 VAL_LOSS: 1.3167996406555176\n",
            "EPOCH: 47 STEP: 23275 VAL_LOSS: 1.3095813989639282\n",
            "EPOCH: 47 STEP: 23300 VAL_LOSS: 1.3154680728912354\n",
            "EPOCH: 47 STEP: 23325 VAL_LOSS: 1.315367341041565\n",
            "EPOCH: 47 STEP: 23350 VAL_LOSS: 1.3068615198135376\n",
            "EPOCH: 47 STEP: 23375 VAL_LOSS: 1.3058936595916748\n",
            "EPOCH: 47 STEP: 23400 VAL_LOSS: 1.309744954109192\n",
            "EPOCH: 47 STEP: 23425 VAL_LOSS: 1.3091676235198975\n",
            "EPOCH: 47 STEP: 23450 VAL_LOSS: 1.3122750520706177\n",
            "EPOCH: 47 STEP: 23475 VAL_LOSS: 1.3061256408691406\n",
            "EPOCH: 47 STEP: 23500 VAL_LOSS: 1.30717134475708\n",
            "EPOCH: 48 STEP: 23525 VAL_LOSS: 1.3063260316848755\n",
            "EPOCH: 48 STEP: 23550 VAL_LOSS: 1.3059208393096924\n",
            "EPOCH: 48 STEP: 23575 VAL_LOSS: 1.308456301689148\n",
            "EPOCH: 48 STEP: 23600 VAL_LOSS: 1.3042442798614502\n",
            "EPOCH: 48 STEP: 23625 VAL_LOSS: 1.3081884384155273\n",
            "EPOCH: 48 STEP: 23650 VAL_LOSS: 1.3068559169769287\n",
            "EPOCH: 48 STEP: 23675 VAL_LOSS: 1.3106963634490967\n",
            "EPOCH: 48 STEP: 23700 VAL_LOSS: 1.3129774332046509\n",
            "EPOCH: 48 STEP: 23725 VAL_LOSS: 1.313950777053833\n",
            "EPOCH: 48 STEP: 23750 VAL_LOSS: 1.311781406402588\n",
            "EPOCH: 48 STEP: 23775 VAL_LOSS: 1.3087176084518433\n",
            "EPOCH: 48 STEP: 23800 VAL_LOSS: 1.3096849918365479\n",
            "EPOCH: 48 STEP: 23825 VAL_LOSS: 1.3138461112976074\n",
            "EPOCH: 48 STEP: 23850 VAL_LOSS: 1.3073103427886963\n",
            "EPOCH: 48 STEP: 23875 VAL_LOSS: 1.3055058717727661\n",
            "EPOCH: 48 STEP: 23900 VAL_LOSS: 1.3035272359848022\n",
            "EPOCH: 48 STEP: 23925 VAL_LOSS: 1.3039963245391846\n",
            "EPOCH: 48 STEP: 23950 VAL_LOSS: 1.3048466444015503\n",
            "EPOCH: 48 STEP: 23975 VAL_LOSS: 1.3036651611328125\n",
            "EPOCH: 48 STEP: 24000 VAL_LOSS: 1.307944416999817\n",
            "EPOCH: 49 STEP: 24025 VAL_LOSS: 1.3001766204833984\n",
            "EPOCH: 49 STEP: 24050 VAL_LOSS: 1.3034894466400146\n",
            "EPOCH: 49 STEP: 24075 VAL_LOSS: 1.3037192821502686\n",
            "EPOCH: 49 STEP: 24100 VAL_LOSS: 1.3046703338623047\n",
            "EPOCH: 49 STEP: 24125 VAL_LOSS: 1.3079439401626587\n",
            "EPOCH: 49 STEP: 24150 VAL_LOSS: 1.306161642074585\n",
            "EPOCH: 49 STEP: 24175 VAL_LOSS: 1.30637788772583\n",
            "EPOCH: 49 STEP: 24200 VAL_LOSS: 1.3046464920043945\n",
            "EPOCH: 49 STEP: 24225 VAL_LOSS: 1.3024871349334717\n",
            "EPOCH: 49 STEP: 24250 VAL_LOSS: 1.3050838708877563\n",
            "EPOCH: 49 STEP: 24275 VAL_LOSS: 1.3043615818023682\n",
            "EPOCH: 49 STEP: 24300 VAL_LOSS: 1.305557131767273\n",
            "EPOCH: 49 STEP: 24325 VAL_LOSS: 1.3085836172103882\n",
            "EPOCH: 49 STEP: 24350 VAL_LOSS: 1.3051828145980835\n",
            "EPOCH: 49 STEP: 24375 VAL_LOSS: 1.3079067468643188\n",
            "EPOCH: 49 STEP: 24400 VAL_LOSS: 1.3147306442260742\n",
            "EPOCH: 49 STEP: 24425 VAL_LOSS: 1.3153995275497437\n",
            "EPOCH: 49 STEP: 24450 VAL_LOSS: 1.3139636516571045\n",
            "EPOCH: 49 STEP: 24475 VAL_LOSS: 1.3147540092468262\n",
            "EPOCH: 49 STEP: 24500 VAL_LOSS: 1.3124148845672607\n",
            "EPOCH: 50 STEP: 24525 VAL_LOSS: 1.3114824295043945\n",
            "EPOCH: 50 STEP: 24550 VAL_LOSS: 1.315458059310913\n",
            "EPOCH: 50 STEP: 24575 VAL_LOSS: 1.3107770681381226\n",
            "EPOCH: 50 STEP: 24600 VAL_LOSS: 1.3092491626739502\n",
            "EPOCH: 50 STEP: 24625 VAL_LOSS: 1.3083391189575195\n",
            "EPOCH: 50 STEP: 24650 VAL_LOSS: 1.3048487901687622\n",
            "EPOCH: 50 STEP: 24675 VAL_LOSS: 1.3042510747909546\n",
            "EPOCH: 50 STEP: 24700 VAL_LOSS: 1.3094160556793213\n",
            "EPOCH: 50 STEP: 24725 VAL_LOSS: 1.306569218635559\n",
            "EPOCH: 50 STEP: 24750 VAL_LOSS: 1.3036518096923828\n",
            "EPOCH: 50 STEP: 24775 VAL_LOSS: 1.314815878868103\n",
            "EPOCH: 50 STEP: 24800 VAL_LOSS: 1.3062280416488647\n",
            "EPOCH: 50 STEP: 24825 VAL_LOSS: 1.3022358417510986\n",
            "EPOCH: 50 STEP: 24850 VAL_LOSS: 1.3088891506195068\n",
            "EPOCH: 50 STEP: 24875 VAL_LOSS: 1.3055466413497925\n",
            "EPOCH: 50 STEP: 24900 VAL_LOSS: 1.3070265054702759\n",
            "EPOCH: 50 STEP: 24925 VAL_LOSS: 1.3050187826156616\n",
            "EPOCH: 50 STEP: 24950 VAL_LOSS: 1.3057950735092163\n",
            "EPOCH: 50 STEP: 24975 VAL_LOSS: 1.3051260709762573\n",
            "EPOCH: 51 STEP: 25000 VAL_LOSS: 1.3037192821502686\n",
            "EPOCH: 51 STEP: 25025 VAL_LOSS: 1.3028022050857544\n",
            "EPOCH: 51 STEP: 25050 VAL_LOSS: 1.3038716316223145\n",
            "EPOCH: 51 STEP: 25075 VAL_LOSS: 1.301514983177185\n",
            "EPOCH: 51 STEP: 25100 VAL_LOSS: 1.304534912109375\n",
            "EPOCH: 51 STEP: 25125 VAL_LOSS: 1.2981706857681274\n",
            "EPOCH: 51 STEP: 25150 VAL_LOSS: 1.3041906356811523\n",
            "EPOCH: 51 STEP: 25175 VAL_LOSS: 1.3031936883926392\n",
            "EPOCH: 51 STEP: 25200 VAL_LOSS: 1.3073086738586426\n",
            "EPOCH: 51 STEP: 25225 VAL_LOSS: 1.310991883277893\n",
            "EPOCH: 51 STEP: 25250 VAL_LOSS: 1.308780312538147\n",
            "EPOCH: 51 STEP: 25275 VAL_LOSS: 1.3107972145080566\n",
            "EPOCH: 51 STEP: 25300 VAL_LOSS: 1.3136608600616455\n",
            "EPOCH: 51 STEP: 25325 VAL_LOSS: 1.303428292274475\n",
            "EPOCH: 51 STEP: 25350 VAL_LOSS: 1.3075138330459595\n",
            "EPOCH: 51 STEP: 25375 VAL_LOSS: 1.3096727132797241\n",
            "EPOCH: 51 STEP: 25400 VAL_LOSS: 1.3108797073364258\n",
            "EPOCH: 51 STEP: 25425 VAL_LOSS: 1.310921311378479\n",
            "EPOCH: 51 STEP: 25450 VAL_LOSS: 1.3089631795883179\n",
            "EPOCH: 51 STEP: 25475 VAL_LOSS: 1.3091285228729248\n",
            "EPOCH: 52 STEP: 25500 VAL_LOSS: 1.3061087131500244\n",
            "EPOCH: 52 STEP: 25525 VAL_LOSS: 1.3040779829025269\n",
            "EPOCH: 52 STEP: 25550 VAL_LOSS: 1.3070430755615234\n",
            "EPOCH: 52 STEP: 25575 VAL_LOSS: 1.2998406887054443\n",
            "EPOCH: 52 STEP: 25600 VAL_LOSS: 1.3053873777389526\n",
            "EPOCH: 52 STEP: 25625 VAL_LOSS: 1.3027890920639038\n",
            "EPOCH: 52 STEP: 25650 VAL_LOSS: 1.3051055669784546\n",
            "EPOCH: 52 STEP: 25675 VAL_LOSS: 1.3103572130203247\n",
            "EPOCH: 52 STEP: 25700 VAL_LOSS: 1.3094408512115479\n",
            "EPOCH: 52 STEP: 25725 VAL_LOSS: 1.30770742893219\n",
            "EPOCH: 52 STEP: 25750 VAL_LOSS: 1.3113460540771484\n",
            "EPOCH: 52 STEP: 25775 VAL_LOSS: 1.3092693090438843\n",
            "EPOCH: 52 STEP: 25800 VAL_LOSS: 1.3018698692321777\n",
            "EPOCH: 52 STEP: 25825 VAL_LOSS: 1.3001062870025635\n",
            "EPOCH: 52 STEP: 25850 VAL_LOSS: 1.3061375617980957\n",
            "EPOCH: 52 STEP: 25875 VAL_LOSS: 1.3041322231292725\n",
            "EPOCH: 52 STEP: 25900 VAL_LOSS: 1.308411717414856\n",
            "EPOCH: 52 STEP: 25925 VAL_LOSS: 1.3025684356689453\n",
            "EPOCH: 52 STEP: 25950 VAL_LOSS: 1.3068736791610718\n",
            "EPOCH: 53 STEP: 25975 VAL_LOSS: 1.307481050491333\n",
            "EPOCH: 53 STEP: 26000 VAL_LOSS: 1.301051139831543\n",
            "EPOCH: 53 STEP: 26025 VAL_LOSS: 1.3052854537963867\n",
            "EPOCH: 53 STEP: 26050 VAL_LOSS: 1.3077442646026611\n",
            "EPOCH: 53 STEP: 26075 VAL_LOSS: 1.3142433166503906\n",
            "EPOCH: 53 STEP: 26100 VAL_LOSS: 1.3092756271362305\n",
            "EPOCH: 53 STEP: 26125 VAL_LOSS: 1.3087328672409058\n",
            "EPOCH: 53 STEP: 26150 VAL_LOSS: 1.3122742176055908\n",
            "EPOCH: 53 STEP: 26175 VAL_LOSS: 1.3090183734893799\n",
            "EPOCH: 53 STEP: 26200 VAL_LOSS: 1.3118934631347656\n",
            "EPOCH: 53 STEP: 26225 VAL_LOSS: 1.3074700832366943\n",
            "EPOCH: 53 STEP: 26250 VAL_LOSS: 1.3157391548156738\n",
            "EPOCH: 53 STEP: 26275 VAL_LOSS: 1.3057633638381958\n",
            "EPOCH: 53 STEP: 26300 VAL_LOSS: 1.3042521476745605\n",
            "EPOCH: 53 STEP: 26325 VAL_LOSS: 1.308011770248413\n",
            "EPOCH: 53 STEP: 26350 VAL_LOSS: 1.3048293590545654\n",
            "EPOCH: 53 STEP: 26375 VAL_LOSS: 1.307316780090332\n",
            "EPOCH: 53 STEP: 26400 VAL_LOSS: 1.306904673576355\n",
            "EPOCH: 53 STEP: 26425 VAL_LOSS: 1.3037911653518677\n",
            "EPOCH: 53 STEP: 26450 VAL_LOSS: 1.305761456489563\n",
            "EPOCH: 54 STEP: 26475 VAL_LOSS: 1.3025050163269043\n",
            "EPOCH: 54 STEP: 26500 VAL_LOSS: 1.3010964393615723\n",
            "EPOCH: 54 STEP: 26525 VAL_LOSS: 1.306921124458313\n",
            "EPOCH: 54 STEP: 26550 VAL_LOSS: 1.3063727617263794\n",
            "EPOCH: 54 STEP: 26575 VAL_LOSS: 1.307178020477295\n",
            "EPOCH: 54 STEP: 26600 VAL_LOSS: 1.2989219427108765\n",
            "EPOCH: 54 STEP: 26625 VAL_LOSS: 1.303454875946045\n",
            "EPOCH: 54 STEP: 26650 VAL_LOSS: 1.30680251121521\n",
            "EPOCH: 54 STEP: 26675 VAL_LOSS: 1.3056995868682861\n",
            "EPOCH: 54 STEP: 26700 VAL_LOSS: 1.3056453466415405\n",
            "EPOCH: 54 STEP: 26725 VAL_LOSS: 1.3018594980239868\n",
            "EPOCH: 54 STEP: 26750 VAL_LOSS: 1.3056824207305908\n",
            "EPOCH: 54 STEP: 26775 VAL_LOSS: 1.3071019649505615\n",
            "EPOCH: 54 STEP: 26800 VAL_LOSS: 1.305435061454773\n",
            "EPOCH: 54 STEP: 26825 VAL_LOSS: 1.3090612888336182\n",
            "EPOCH: 54 STEP: 26850 VAL_LOSS: 1.3104709386825562\n",
            "EPOCH: 54 STEP: 26875 VAL_LOSS: 1.3119099140167236\n",
            "EPOCH: 54 STEP: 26900 VAL_LOSS: 1.307352900505066\n",
            "EPOCH: 54 STEP: 26925 VAL_LOSS: 1.3098043203353882\n",
            "EPOCH: 54 STEP: 26950 VAL_LOSS: 1.3100996017456055\n",
            "EPOCH: 55 STEP: 26975 VAL_LOSS: 1.3065311908721924\n",
            "EPOCH: 55 STEP: 27000 VAL_LOSS: 1.30614173412323\n",
            "EPOCH: 55 STEP: 27025 VAL_LOSS: 1.3131046295166016\n",
            "EPOCH: 55 STEP: 27050 VAL_LOSS: 1.312100887298584\n",
            "EPOCH: 55 STEP: 27075 VAL_LOSS: 1.3084630966186523\n",
            "EPOCH: 55 STEP: 27100 VAL_LOSS: 1.3062524795532227\n",
            "EPOCH: 55 STEP: 27125 VAL_LOSS: 1.2998024225234985\n",
            "EPOCH: 55 STEP: 27150 VAL_LOSS: 1.307536244392395\n",
            "EPOCH: 55 STEP: 27175 VAL_LOSS: 1.3070099353790283\n",
            "EPOCH: 55 STEP: 27200 VAL_LOSS: 1.3024760484695435\n",
            "EPOCH: 55 STEP: 27225 VAL_LOSS: 1.3067972660064697\n",
            "EPOCH: 55 STEP: 27250 VAL_LOSS: 1.309570550918579\n",
            "EPOCH: 55 STEP: 27275 VAL_LOSS: 1.3040130138397217\n",
            "EPOCH: 55 STEP: 27300 VAL_LOSS: 1.3074891567230225\n",
            "EPOCH: 55 STEP: 27325 VAL_LOSS: 1.3049579858779907\n",
            "EPOCH: 55 STEP: 27350 VAL_LOSS: 1.3037737607955933\n",
            "EPOCH: 55 STEP: 27375 VAL_LOSS: 1.3043771982192993\n",
            "EPOCH: 55 STEP: 27400 VAL_LOSS: 1.298048734664917\n",
            "EPOCH: 55 STEP: 27425 VAL_LOSS: 1.3064756393432617\n",
            "EPOCH: 56 STEP: 27450 VAL_LOSS: 1.305064082145691\n",
            "EPOCH: 56 STEP: 27475 VAL_LOSS: 1.3099937438964844\n",
            "EPOCH: 56 STEP: 27500 VAL_LOSS: 1.309144139289856\n",
            "EPOCH: 56 STEP: 27525 VAL_LOSS: 1.30806303024292\n",
            "EPOCH: 56 STEP: 27550 VAL_LOSS: 1.3036305904388428\n",
            "EPOCH: 56 STEP: 27575 VAL_LOSS: 1.306761384010315\n",
            "EPOCH: 56 STEP: 27600 VAL_LOSS: 1.3082317113876343\n",
            "EPOCH: 56 STEP: 27625 VAL_LOSS: 1.3060861825942993\n",
            "EPOCH: 56 STEP: 27650 VAL_LOSS: 1.3088247776031494\n",
            "EPOCH: 56 STEP: 27675 VAL_LOSS: 1.3048962354660034\n",
            "EPOCH: 56 STEP: 27700 VAL_LOSS: 1.3013392686843872\n",
            "EPOCH: 56 STEP: 27725 VAL_LOSS: 1.3044376373291016\n",
            "EPOCH: 56 STEP: 27750 VAL_LOSS: 1.3058220148086548\n",
            "EPOCH: 56 STEP: 27775 VAL_LOSS: 1.302804946899414\n",
            "EPOCH: 56 STEP: 27800 VAL_LOSS: 1.3056275844573975\n",
            "EPOCH: 56 STEP: 27825 VAL_LOSS: 1.3061423301696777\n",
            "EPOCH: 56 STEP: 27850 VAL_LOSS: 1.303960919380188\n",
            "EPOCH: 56 STEP: 27875 VAL_LOSS: 1.3020665645599365\n",
            "EPOCH: 56 STEP: 27900 VAL_LOSS: 1.3050782680511475\n",
            "EPOCH: 56 STEP: 27925 VAL_LOSS: 1.3021612167358398\n",
            "EPOCH: 57 STEP: 27950 VAL_LOSS: 1.302828073501587\n",
            "EPOCH: 57 STEP: 27975 VAL_LOSS: 1.3055741786956787\n",
            "EPOCH: 57 STEP: 28000 VAL_LOSS: 1.308118224143982\n",
            "EPOCH: 57 STEP: 28025 VAL_LOSS: 1.308825135231018\n",
            "EPOCH: 57 STEP: 28050 VAL_LOSS: 1.3127751350402832\n",
            "EPOCH: 57 STEP: 28075 VAL_LOSS: 1.3113816976547241\n",
            "EPOCH: 57 STEP: 28100 VAL_LOSS: 1.3086315393447876\n",
            "EPOCH: 57 STEP: 28125 VAL_LOSS: 1.3092219829559326\n",
            "EPOCH: 57 STEP: 28150 VAL_LOSS: 1.3119983673095703\n",
            "EPOCH: 57 STEP: 28175 VAL_LOSS: 1.3041725158691406\n",
            "EPOCH: 57 STEP: 28200 VAL_LOSS: 1.3087618350982666\n",
            "EPOCH: 57 STEP: 28225 VAL_LOSS: 1.3047292232513428\n",
            "EPOCH: 57 STEP: 28250 VAL_LOSS: 1.2996126413345337\n",
            "EPOCH: 57 STEP: 28275 VAL_LOSS: 1.2992632389068604\n",
            "EPOCH: 57 STEP: 28300 VAL_LOSS: 1.3038990497589111\n",
            "EPOCH: 57 STEP: 28325 VAL_LOSS: 1.311734676361084\n",
            "EPOCH: 57 STEP: 28350 VAL_LOSS: 1.3119887113571167\n",
            "EPOCH: 57 STEP: 28375 VAL_LOSS: 1.305556297302246\n",
            "EPOCH: 57 STEP: 28400 VAL_LOSS: 1.3069654703140259\n",
            "EPOCH: 58 STEP: 28425 VAL_LOSS: 1.3048069477081299\n",
            "EPOCH: 58 STEP: 28450 VAL_LOSS: 1.3035203218460083\n",
            "EPOCH: 58 STEP: 28475 VAL_LOSS: 1.310689926147461\n",
            "EPOCH: 58 STEP: 28500 VAL_LOSS: 1.3049814701080322\n",
            "EPOCH: 58 STEP: 28525 VAL_LOSS: 1.313116431236267\n",
            "EPOCH: 58 STEP: 28550 VAL_LOSS: 1.308382272720337\n",
            "EPOCH: 58 STEP: 28575 VAL_LOSS: 1.3065292835235596\n",
            "EPOCH: 58 STEP: 28600 VAL_LOSS: 1.3077434301376343\n",
            "EPOCH: 58 STEP: 28625 VAL_LOSS: 1.30849027633667\n",
            "EPOCH: 58 STEP: 28650 VAL_LOSS: 1.3078069686889648\n",
            "EPOCH: 58 STEP: 28675 VAL_LOSS: 1.3055013418197632\n",
            "EPOCH: 58 STEP: 28700 VAL_LOSS: 1.3094731569290161\n",
            "EPOCH: 58 STEP: 28725 VAL_LOSS: 1.3072978258132935\n",
            "EPOCH: 58 STEP: 28750 VAL_LOSS: 1.3036820888519287\n",
            "EPOCH: 58 STEP: 28775 VAL_LOSS: 1.306717038154602\n",
            "EPOCH: 58 STEP: 28800 VAL_LOSS: 1.3068045377731323\n",
            "EPOCH: 58 STEP: 28825 VAL_LOSS: 1.3101019859313965\n",
            "EPOCH: 58 STEP: 28850 VAL_LOSS: 1.3051176071166992\n",
            "EPOCH: 58 STEP: 28875 VAL_LOSS: 1.302443027496338\n",
            "EPOCH: 58 STEP: 28900 VAL_LOSS: 1.3060694932937622\n",
            "EPOCH: 59 STEP: 28925 VAL_LOSS: 1.303968906402588\n",
            "EPOCH: 59 STEP: 28950 VAL_LOSS: 1.3101996183395386\n",
            "EPOCH: 59 STEP: 28975 VAL_LOSS: 1.2966312170028687\n",
            "EPOCH: 59 STEP: 29000 VAL_LOSS: 1.2980633974075317\n",
            "EPOCH: 59 STEP: 29025 VAL_LOSS: 1.2993695735931396\n",
            "EPOCH: 59 STEP: 29050 VAL_LOSS: 1.300693392753601\n",
            "EPOCH: 59 STEP: 29075 VAL_LOSS: 1.3035976886749268\n",
            "EPOCH: 59 STEP: 29100 VAL_LOSS: 1.301115870475769\n",
            "EPOCH: 59 STEP: 29125 VAL_LOSS: 1.2986222505569458\n",
            "EPOCH: 59 STEP: 29150 VAL_LOSS: 1.2994492053985596\n",
            "EPOCH: 59 STEP: 29175 VAL_LOSS: 1.2974392175674438\n",
            "EPOCH: 59 STEP: 29200 VAL_LOSS: 1.297699213027954\n",
            "EPOCH: 59 STEP: 29225 VAL_LOSS: 1.3011630773544312\n",
            "EPOCH: 59 STEP: 29250 VAL_LOSS: 1.2996022701263428\n",
            "EPOCH: 59 STEP: 29275 VAL_LOSS: 1.300474762916565\n",
            "EPOCH: 59 STEP: 29300 VAL_LOSS: 1.303274154663086\n",
            "EPOCH: 59 STEP: 29325 VAL_LOSS: 1.304988145828247\n",
            "EPOCH: 59 STEP: 29350 VAL_LOSS: 1.3090317249298096\n",
            "EPOCH: 59 STEP: 29375 VAL_LOSS: 1.3081824779510498\n",
            "EPOCH: 59 STEP: 29400 VAL_LOSS: 1.3037713766098022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'hidden512_layers3_shakes.net'"
      ],
      "metadata": {
        "id": "e0LjUnRdyGMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(),model_name)"
      ],
      "metadata": {
        "id": "-6VG17p_6ICY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_char(model,char,hidden=None,k=1):\n",
        "  encoded_text = model.encoder[char]\n",
        "  encoded_text = np.array([[encoded_text]])\n",
        "  encoded_text = one_hot_encoder(encoded_text,len(model.all_chars))\n",
        "  inputs = torch.from_numpy(encoded_text)\n",
        "  if model.use_gpu:\n",
        "    inputs = inputs.cuda()\n",
        "  hidden = tuple([state.data for state in hidden])\n",
        "  lstm_out,hidden = model(inputs,hidden)\n",
        "  probs = F.softmax(lstm_out,dim=1).data\n",
        "  if model.use_gpu:\n",
        "    probs = probs.cpu()\n",
        "  probs,index_positions = probs.topk(k)\n",
        "  index_positions = index_positions.numpy().squeeze()\n",
        "  probs = probs.numpy().flatten()\n",
        "  probs = probs/probs.sum()\n",
        "  char = np.random.choice(index_positions,p=probs)\n",
        "  return model.decoder[char],hidden"
      ],
      "metadata": {
        "id": "SUxOwJE56H1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model,size,seed='The',k=1):\n",
        "  if model.use_gpu:\n",
        "    model.cuda()\n",
        "  else:\n",
        "    model.cpu()\n",
        "  model.eval()\n",
        "  output_chars = [c for c in seed]\n",
        "  hidden = model.hidden_state(1)\n",
        "  for char in seed:\n",
        "    char,hidden = predict_next_char(model,char,hidden,k=k)\n",
        "  output_chars.append(char)\n",
        "  for i in range(size):\n",
        "    char,hidden = predict_next_char(model,output_chars[-1],hidden,k=k)\n",
        "    output_chars.append(char)\n",
        "  return ''.join(output_chars)"
      ],
      "metadata": {
        "id": "AJqlE0EF6HoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model,1000,seed='The ',k=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZ4HOCPVTq6O",
        "outputId": "bf008452-b1c1-42a9-f3d2-ef54630277da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The contempts, the world, the door of the commonwealth, and the sense to the King's\n",
            "\n",
            "    What is't?\n",
            "  CASCA. Well, I'll not have my life as I have dead;\n",
            "    And this was seen with me.\n",
            "  COUNTESS. If you do, my lord,\n",
            "    The watch, that have no passage on my heart,\n",
            "    I would have struck the story to your sister.\n",
            "    Then to the sense of many things are stood.\n",
            "    Then that too much that she's a most affair,\n",
            "    And therefore seek me there against the sense,\n",
            "    And see the service of the state of honour,\n",
            "    And sometime shall I think that the sun that shall have\n",
            "    An exproved monster, and she shall set me with me.\n",
            "    I am an angry, so much are they all.\n",
            "    Thou shalt not, she is some thing to be so for thee,\n",
            "    To be a state as to my soul to thine\n",
            "    As we are as a man till she was thine.\n",
            "    I will be too much seasons to thy son.\n",
            "    I would not think of my design to th' mark,\n",
            "    And will not be a soul to think of this.\n",
            "  BASSANIO. Well, I'll see you to the way. If I should be\n",
            "   \n"
          ]
        }
      ]
    }
  ]
}